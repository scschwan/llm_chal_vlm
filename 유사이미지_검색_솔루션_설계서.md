# 유사이미지 검색 솔루션 설계서

## 프로젝트 개요

기존 `llm_chal_vlm` 코드베이스를 기반으로 **CLIP 기반 유사이미지 검색 → Anomaly Detection → LLM 대응 매뉴얼 생성** 파이프라인을 구축하는 프로젝트입니다.

### 주요 목표
1. **CLIP 유사이미지 검색**: 입력 이미지와 유사한 이미지를 TOP-K 방식으로 검색
2. **차이점 분석**: Anomaly Detection을 활용한 입력 이미지와 유사 이미지 간 차이 영역 검출
3. **대응 매뉴얼 생성**: RAG/LangChain을 활용해 불량 대응 매뉴얼 기반 LLM 답변 생성

---

## 1. 현재 코드베이스 분석

### 1.1 프로젝트 구조
```
llm_chal_vlm/
├── config.py                   # 설정 파일
├── main.py                     # 메인 실행 파일
├── modules/
│   ├── clip_search.py         # CLIP 검색 엔진
│   ├── vlm_local.py           # VLM(Vision Language Model) 래퍼
│   ├── region_detect.py       # ROI 검출 (차이 영역 탐지)
│   ├── ssim_utils.py          # SSIM 유사도 계산
│   ├── image_processor.py     # 이미지 처리 유틸
│   ├── prompts.py             # 프롬프트 템플릿
│   ├── preprocess.py          # 이미지 전처리
│   ├── object_guidance.py     # 객체 가이드
│   └── shape_diff.py          # 형태 차이 분석
├── data/
│   ├── ok_front/              # 정상 이미지 (갤러리)
│   └── def_front/             # 불량 이미지 (쿼리)
└── results/                   # 결과 저장 디렉토리
```

### 1.2 핵심 모듈 분석

#### CLIPSearch (modules/clip_search.py)
- **역할**: CLIP 모델을 사용한 이미지 임베딩 및 유사도 검색
- **주요 기능**:
  - `build_index()`: 갤러리 이미지 벡터 인덱스 구축
  - `search_with_index()`: 쿼리 이미지로 유사 이미지 검색 (TOP-K)
  - FAISS 지원 (설치 시 빠른 검색)
- **현재 설정**: `openai/clip-vit-large-patch14` 모델 사용

#### VLM (modules/vlm_local.py)
- **역할**: 멀티모달 LLM (LLaVA 등) 래퍼
- **주요 기능**:
  - `compare_regions_text()`: 두 이미지 비교 설명 생성
  - `describe_image_json()`: 단일 이미지 JSON 캡션 생성
- **현재 설정**: `llava-hf/llava-v1.6-mistral-7b-hf` 모델

#### RegionDetect (modules/region_detect.py)
- **역할**: 이미지 차이 영역 검출
- **주요 기능**:
  - `diff_mask()`: 두 이미지 차분 마스크 생성
  - `extract_rois()`: ROI(Region of Interest) 추출
  - `overlay_boxes()`: ROI 박스 오버레이 시각화
- **알고리즘**: OpenCV 기반 차분 + 컨투어 검출 + NMS

---

## 2. 새로운 솔루션 설계

### 2.1 전체 아키텍처

```
[입력 이미지] 
    ↓
[CLIP 임베딩]
    ↓
[유사 이미지 검색 (TOP-K)]
    ↓
[UI 표시: 좌측(입력), 우측(유사 이미지 리스트)]
    ↓
[Anomaly Detection / YOLO 차이 분석]
    ↓
[차이 영역 Segmentation & 시각화]
    ↓
[RAG: PDF 매뉴얼 검색]
    ↓
[LLM: 대응 방안 생성]
    ↓
[하단 로그 터미널 출력]
```

### 2.2 기능별 상세 설계

---

## 3. 기능 1: CLIP 기반 유사이미지 검색 (TOP-K)

### 3.1 요구사항
- 입력 이미지와 벡터화된 데이터베이스에서 유사 이미지 검색
- TOP-K 개수는 설정 가능 (기본값: 3)
- 가장 유사도 높은 이미지 1장은 크게 표시, 나머지는 작은 썸네일로 표시
- 썸네일 클릭 시 메인 이미지와 스왑 (쇼핑몰 상품 상세 이미지 방식)

### 3.2 구현 계획

#### 3.2.1 백엔드 (검색 엔진)
```python
# modules/clip_search.py 활용 (기존 코드 그대로 사용 가능)

clip_searcher = CLIPSearch(
    model_id="openai/clip-vit-large-patch14",
    device="cuda",
    verbose=True
)

# 인덱스 구축 (정상 이미지 데이터베이스)
clip_searcher.build_index("data/ok_front/")

# 검색 (입력 이미지)
top_k_results = clip_searcher.search_with_index(
    query_img_path="input_image.jpg",
    top_k=3
)

# 결과: List[ClipHit]
# ClipHit.candidate_path: 유사 이미지 경로
# ClipHit.similarity: 코사인 유사도 (0~1)
```

#### 3.2.2 프론트엔드 (UI 레이아웃)
**레이아웃 구조**:
```
┌─────────────────────────────────────────────────┐
│  [입력 이미지]  │  [유사 이미지 메인 (TOP-1)]  │
│      (고정)      │          (클릭 가능)         │
│                  │                               │
│                  ├───────────────────────────────┤
│                  │  [썸네일 2] [썸네일 3] ...   │
│                  │   (클릭 시 메인과 스왑)      │
└─────────────────────────────────────────────────┘
│           [로그 터미널 영역]                    │
└─────────────────────────────────────────────────┘
```

**UI 구현 방안** (Gradio/Streamlit 사용 권장):

```python
import gradio as gr

def search_similar_images(input_image, top_k=3):
    # CLIP 검색
    results = clip_searcher.search_with_index(input_image, top_k)
    
    # 메인 이미지 (TOP-1)
    main_image = Image.open(results[0].candidate_path)
    main_similarity = results[0].similarity
    
    # 썸네일 리스트 (TOP-2 ~ TOP-K)
    thumbnails = [(Image.open(r.candidate_path), r.similarity) 
                  for r in results[1:]]
    
    return main_image, main_similarity, thumbnails

# Gradio UI
with gr.Blocks() as demo:
    with gr.Row():
        input_img = gr.Image(label="입력 이미지", type="filepath")
        with gr.Column():
            main_img = gr.Image(label="가장 유사한 이미지")
            similarity_text = gr.Textbox(label="유사도")
    
    with gr.Row():
        thumbnail_gallery = gr.Gallery(label="기타 유사 이미지")
    
    log_terminal = gr.Textbox(label="분석 로그", lines=10)
    
    input_img.change(
        search_similar_images,
        inputs=[input_img],
        outputs=[main_img, similarity_text, thumbnail_gallery]
    )

demo.launch()
```

#### 3.2.3 이미지 스왑 기능
```python
def swap_images(clicked_thumbnail_index):
    """썸네일 클릭 시 메인 이미지와 위치 교체"""
    global current_results  # 현재 검색 결과 저장
    
    # 현재 메인 이미지 (TOP-1)
    current_main = current_results[0]
    
    # 클릭된 썸네일
    clicked_image = current_results[clicked_thumbnail_index]
    
    # 위치 교환
    current_results[0] = clicked_image
    current_results[clicked_thumbnail_index] = current_main
    
    # UI 업데이트
    return update_ui(current_results)
```

### 3.3 개선 사항
- **캐싱**: 인덱스 빌드는 시간이 오래 걸리므로 사전 구축 후 저장
- **성능 최적화**: FAISS 사용 (기존 코드에 이미 구현됨)
- **배치 검색**: 여러 입력 이미지 동시 처리

---

## 4. 기능 2: Anomaly Detection 기반 차이 분석

### 4.1 요구사항
- 입력 이미지와 유사 이미지 간 차이 영역 검출
- 차이 영역을 Segmentation하여 시각화
- OpenCV로 바운딩 박스 또는 마스크 표시

### 4.2 현재 코드 활용
기존 `modules/region_detect.py`의 `extract_rois()` 함수가 이미 차이 검출 기능을 구현하고 있습니다.

```python
# 기존 코드 활용 예제
from modules.region_detect import extract_rois, overlay_boxes

# ROI 추출
rois, roi_stats = extract_rois(
    left_img=input_image,
    right_img=similar_image,
    topk=5,                     # 최대 5개 ROI
    min_area_ratio=0.001,       # 전체 면적의 0.1% 이상
    max_area_ratio=0.20,        # 전체 면적의 20% 이하
    pad_ratio=0.06,             # 박스 확장 비율
    iou_thr=0.5,                # NMS 임계값
    max_coverage_ratio=0.25,    # 전체 커버리지 25% 제한
    thresh_mode="auto"          # Otsu 자동 임계값
)

# 시각화
input_with_boxes = overlay_boxes(input_image, rois, color=(0, 255, 0), width=3)
similar_with_boxes = overlay_boxes(similar_image, rois, color=(255, 0, 0), width=3)
```

### 4.3 알고리즘 상세

#### 4.3.1 Difference Mask 생성
```python
# modules/region_detect.py의 diff_mask() 함수
def diff_mask(left, right, blur_ksize=3, thresh_mode="auto"):
    """
    1. 그레이스케일 변환
    2. 절대 차분 (cv2.absdiff)
    3. 가우시안 블러
    4. 이진화 (Otsu 또는 고정 임계값)
    5. 모폴로지 연산 (노이즈 제거)
    """
    g1 = cv2.cvtColor(np.array(left), cv2.COLOR_RGB2GRAY)
    g2 = cv2.cvtColor(np.array(right), cv2.COLOR_RGB2GRAY)
    d = cv2.absdiff(g1, g2)
    d = cv2.GaussianBlur(d, (blur_ksize, blur_ksize), 0)
    
    if thresh_mode == "auto":
        _, mask = cv2.threshold(d, 0, 255, cv2.THRESH_OTSU)
    else:
        _, mask = cv2.threshold(d, thresh_val, 255, cv2.THRESH_BINARY)
    
    # 모폴로지 연산
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3)))
    mask = cv2.morphologyEx(mask, cv2.MORPH_DILATE, np.ones((3,3)))
    return mask
```

#### 4.3.2 ROI 추출 및 필터링
```python
# 컨투어 검출
contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 박스 추출 및 필터링
boxes = []
for contour in contours:
    x, y, w, h = cv2.boundingRect(contour)
    area = w * h
    
    # 면적 필터링
    if min_area < area < max_area:
        boxes.append(Box(x, y, w, h, score=area))

# NMS (중복 제거)
boxes = non_max_suppression(boxes, iou_threshold=0.5)

# TOP-K 선택
boxes = sorted(boxes, key=lambda b: b.score, reverse=True)[:topk]
```

### 4.4 대체 방안: YOLO Segmentation

현재 코드는 전통적인 CV 기법을 사용하지만, 성능이 부족할 경우 YOLO 세그멘테이션으로 대체 가능합니다.

```python
# modules/evaluate_yolo_seg.py 참고
from ultralytics import YOLO

# YOLO 세그멘테이션 모델 로드
model = YOLO("yolov8n-seg.pt")

# 예측
results = model(input_image)

# 세그멘테이션 마스크 추출
for result in results:
    masks = result.masks.data  # [N, H, W] 텐서
    boxes = result.boxes.xyxy  # [N, 4] 바운딩 박스
```

**YOLO 사용 시 장점**:
- 더 정확한 객체 검출
- 사전 학습된 가중치 활용
- 실시간 처리 가능

**YOLO 사용 시 고려사항**:
- 도메인 특화 데이터로 파인튜닝 필요할 수 있음
- 모델 크기 및 추론 속도 트레이드오프

### 4.5 개선 사항
- **하이퍼파라미터 튜닝**: `thresh_mode`, `min_area_ratio` 등을 데이터셋에 맞게 조정
- **다중 스케일 검출**: 피라미드 방식으로 다양한 크기 차이 검출
- **시간적 안정성**: 연속 프레임에서 안정적인 ROI 추적

---

## 5. 기능 3: RAG 기반 LLM 대응 매뉴얼 생성

### 5.1 요구사항
- PDF 불량 대응 매뉴얼 문서 준비
- LangChain + RAG를 활용한 관련 문서 검색
- LLaVA 또는 EXAONE 같은 오픈소스 LLM에 전달
- 생성된 답변을 로그 터미널에 출력

### 5.2 RAG 파이프라인 구축

#### 5.2.1 PDF 문서 준비
```
manuals/
├── 표면_불량_대응_매뉴얼.pdf
├── 형상_불량_대응_매뉴얼.pdf
└── 치수_불량_대응_매뉴얼.pdf
```

#### 5.2.2 문서 임베딩 및 벡터 DB 구축
```python
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# 1. PDF 로드
loader = PyPDFLoader("manuals/표면_불량_대응_매뉴얼.pdf")
documents = loader.load()

# 2. 텍스트 분할
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
texts = text_splitter.split_documents(documents)

# 3. 임베딩 모델
embeddings = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sbert-nli",  # 한국어 임베딩 모델
    model_kwargs={'device': 'cuda'}
)

# 4. 벡터 DB 구축
vectorstore = FAISS.from_documents(texts, embeddings)

# 5. 저장 (재사용)
vectorstore.save_local("manual_vectorstore")
```

#### 5.2.3 검색 및 컨텍스트 생성
```python
# 벡터 DB 로드
vectorstore = FAISS.load_local(
    "manual_vectorstore",
    embeddings=HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")
)

# 쿼리 생성 (차이 분석 결과 기반)
query = f"""
입력 이미지와 유사 이미지 간 차이점:
- ROI 1: x={roi1.x}, y={roi1.y}, 면적={roi1.area}px (표면 변색 의심)
- ROI 2: x={roi2.x}, y={roi2.y}, 면적={roi2.area}px (형상 변형 의심)

이러한 불량에 대한 대응 방안을 알려주세요.
"""

# 관련 문서 검색 (TOP-K=3)
relevant_docs = vectorstore.similarity_search(query, k=3)

# 컨텍스트 구성
context = "\n\n".join([doc.page_content for doc in relevant_docs])
```

#### 5.2.4 LLM 프롬프트 구성
```python
# 기존 modules/prompts.py 활용 및 확장
from modules.prompts import build_ok_def_pair_prompt

# 베이스 프롬프트 (이미지 비교)
base_prompt = build_ok_def_pair_prompt(
    evidence_summary=f"CLIP={similarity:.3f}, SSIM={ssim:.3f}",
    roi_hint=roi_hints,
    grid_hints=grid_hints,
    hotspots=hotspots,
    defect_level="중간",
    cfg=PromptConfig(...)
)

# RAG 컨텍스트 추가
rag_prompt = f"""
{base_prompt}

[참조 매뉴얼]
{context}

위 매뉴얼을 참고하여 불량 원인과 대응 방안을 구체적으로 제시하세요.
"""
```

#### 5.2.5 LLM 호출 (LLaVA 또는 EXAONE)
```python
# 기존 modules/vlm_local.py의 VLM 클래스 활용

# LLaVA 사용 예제
vlm = VLM(
    model_id="llava-hf/llava-v1.6-mistral-7b-hf",
    device="cuda",
    use_bf16=True,
    verbose=True
)

# 이미지 + 프롬프트로 답변 생성
response = vlm.compare_regions_text(
    left_path=input_image_path,
    right_path=similar_image_path,
    prompt=rag_prompt,
    max_new_tokens=512,
    temperature=0.7
)

print(response)
```

**EXAONE 사용 시**:
```python
# EXAONE은 한국어에 강점이 있으므로 고려 가능
# https://huggingface.co/LGAI-EXAONE

vlm = VLM(
    model_id="LGAI-EXAONE/EXAONE-3.5-VL",  # 예시
    device="cuda",
    use_bf16=True
)
```

### 5.3 로그 터미널 출력
```python
# Gradio UI에 로그 터미널 추가
log_output = gr.Textbox(
    label="분석 결과 및 대응 방안",
    lines=15,
    max_lines=30,
    interactive=False
)

def update_log(message):
    """로그 메시지 추가"""
    current_log = log_output.value or ""
    timestamp = datetime.now().strftime("%H:%M:%S")
    new_log = f"[{timestamp}] {message}\n{current_log}"
    return new_log

# 각 단계마다 로그 업데이트
update_log("CLIP 검색 완료: TOP-3 유사 이미지 검색됨")
update_log("차이 분석 완료: 5개 ROI 검출")
update_log("매뉴얼 검색 완료: 3개 관련 문서 발견")
update_log(f"LLM 답변: {response}")
```

### 5.4 개선 사항
- **재순위화(Reranking)**: 검색된 문서를 재정렬하여 관련성 높은 문서 우선 사용
- **하이브리드 검색**: 키워드 검색과 벡터 검색 결합
- **프롬프트 엔지니어링**: Few-shot 예제 추가로 답변 품질 향상
- **스트리밍**: LLM 답변을 실시간으로 터미널에 출력

---

## 6. 통합 워크플로우

### 6.1 전체 실행 흐름

```python
class SimilarImageInspectionSystem:
    def __init__(self):
        # 1. CLIP 검색 엔진 초기화
        self.clip_searcher = CLIPSearch(
            model_id="openai/clip-vit-large-patch14",
            device="cuda"
        )
        self.clip_searcher.build_index("data/ok_front/")
        
        # 2. VLM 초기화
        self.vlm = VLM(
            model_id="llava-hf/llava-v1.6-mistral-7b-hf",
            device="cuda"
        )
        
        # 3. RAG 벡터 DB 로드
        self.vectorstore = FAISS.load_local(
            "manual_vectorstore",
            embeddings=HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")
        )
    
    def process(self, input_image_path, top_k=3):
        """전체 파이프라인 실행"""
        
        # Step 1: 유사 이미지 검색
        results = self.clip_searcher.search_with_index(
            input_image_path, 
            top_k=top_k
        )
        main_image_path = results[0].candidate_path
        similarity = results[0].similarity
        
        yield f"✅ 유사 이미지 검색 완료 (유사도: {similarity:.4f})"
        
        # Step 2: 차이 분석
        input_img = Image.open(input_image_path)
        similar_img = Image.open(main_image_path)
        
        rois, roi_stats = extract_rois(input_img, similar_img, topk=5)
        
        yield f"✅ 차이 분석 완료 ({roi_stats['roi_count']}개 ROI 검출)"
        
        # Step 3: 시각화
        input_with_boxes = overlay_boxes(input_img, rois, color=(0, 255, 0))
        similar_with_boxes = overlay_boxes(similar_img, rois, color=(255, 0, 0))
        
        # Step 4: RAG 검색
        roi_description = boxes_to_prompt_hints(rois, "ROI", roi_stats["total_px"])
        query = f"차이 영역: {', '.join(roi_description)}"
        
        relevant_docs = self.vectorstore.similarity_search(query, k=3)
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        
        yield f"✅ 매뉴얼 검색 완료 ({len(relevant_docs)}개 문서)"
        
        # Step 5: LLM 답변 생성
        prompt = self._build_prompt(similarity, rois, roi_stats, context)
        
        response = self.vlm.compare_regions_text(
            input_image_path,
            main_image_path,
            prompt=prompt,
            max_new_tokens=512
        )
        
        yield f"✅ LLM 답변 생성 완료\n\n{response}"
        
        # 결과 반환
        return {
            "main_image": main_image_path,
            "similarity": similarity,
            "thumbnails": [r.candidate_path for r in results[1:]],
            "input_with_boxes": input_with_boxes,
            "similar_with_boxes": similar_with_boxes,
            "rois": rois,
            "response": response
        }
    
    def _build_prompt(self, similarity, rois, roi_stats, context):
        """프롬프트 구성"""
        evidence = f"CLIP={similarity:.3f}, ROI={roi_stats['roi_count']}개"
        roi_hints = boxes_to_prompt_hints(rois, "ROI", roi_stats["total_px"])
        
        base_prompt = build_ok_def_pair_prompt(
            evidence_summary=evidence,
            roi_hint="; ".join(roi_hints),
            grid_hints=[],
            hotspots=[],
            defect_level="분석 필요",
            cfg=PromptConfig(language="ko")
        )
        
        return f"""
{base_prompt}

[참조 매뉴얼]
{context}

위 매뉴얼을 참고하여 불량 원인과 구체적인 대응 방안을 제시하세요.
"""
```

### 6.2 Gradio UI 통합

```python
import gradio as gr

system = SimilarImageInspectionSystem()

def run_inspection(input_image, top_k):
    """UI 콜백 함수"""
    log_messages = []
    
    for message in system.process(input_image, top_k):
        log_messages.append(message)
        yield {
            log_terminal: "\n".join(log_messages)
        }

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# 유사이미지 검색 및 불량 분석 시스템")
    
    with gr.Row():
        # 좌측: 입력 이미지
        with gr.Column(scale=1):
            input_img = gr.Image(
                label="입력 이미지 (불량 의심)",
                type="filepath"
            )
            top_k_slider = gr.Slider(
                minimum=1,
                maximum=10,
                value=3,
                step=1,
                label="유사 이미지 개수 (TOP-K)"
            )
            run_btn = gr.Button("분석 시작", variant="primary")
        
        # 우측: 유사 이미지
        with gr.Column(scale=2):
            main_similar = gr.Image(label="가장 유사한 이미지")
            similarity_score = gr.Textbox(label="유사도 점수")
            thumbnail_gallery = gr.Gallery(
                label="기타 유사 이미지 (클릭하여 교체)",
                columns=3,
                height=200
            )
    
    # 차이 분석 결과
    with gr.Row():
        input_with_roi = gr.Image(label="입력 이미지 (ROI 표시)")
        similar_with_roi = gr.Image(label="유사 이미지 (ROI 표시)")
    
    # 로그 터미널
    log_terminal = gr.Textbox(
        label="분석 로그 및 대응 방안",
        lines=15,
        max_lines=30,
        interactive=False
    )
    
    # 이벤트 바인딩
    run_btn.click(
        fn=run_inspection,
        inputs=[input_img, top_k_slider],
        outputs=[log_terminal]
    )

demo.launch(share=True)
```

---

## 7. 데이터 준비

### 7.1 이미지 데이터
```
data/
├── ok_front/           # 정상 이미지 (갤러리)
│   ├── ok_001.jpg
│   ├── ok_002.jpg
│   └── ...
├── def_front/          # 불량 이미지 (쿼리용)
│   ├── def_001.jpg
│   ├── def_002.jpg
│   └── ...
└── fixed_objects.json  # 고정 관심 객체 (선택)
```

### 7.2 매뉴얼 문서
```
manuals/
├── 표면_불량_대응_매뉴얼.pdf
├── 형상_불량_대응_매뉴얼.pdf
├── 치수_불량_대응_매뉴얼.pdf
└── 기타_불량_대응_매뉴얼.pdf
```

### 7.3 벡터 DB (사전 구축)
```bash
# 벡터 DB 구축 스크립트
python scripts/build_manual_vectorstore.py \
    --manual_dir manuals/ \
    --output_dir manual_vectorstore/ \
    --embedding_model jhgan/ko-sbert-nli
```

---

## 8. 성능 최적화

### 8.1 CLIP 검색 최적화
- **FAISS 인덱스**: 이미 구현됨 (`modules/clip_search.py`)
- **GPU 가속**: `device="cuda"` 설정
- **배치 처리**: 여러 이미지 동시 임베딩

### 8.2 차이 분석 최적화
- **해상도 조정**: 큰 이미지는 리사이즈 후 처리
- **ROI 캐싱**: 동일 이미지 쌍은 결과 재사용
- **YOLO 경량 모델**: `yolov8n-seg` 사용

### 8.3 LLM 최적화
- **양자화**: 4-bit 또는 8-bit 양자화 (`bitsandbytes`)
- **프롬프트 캐싱**: 공통 프롬프트 재사용
- **배치 추론**: 여러 요청 동시 처리

```python
# 양자화 예제
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

vlm = VLM(
    model_id="llava-hf/llava-v1.6-mistral-7b-hf",
    quantization_config=quantization_config
)
```

---

## 9. 테스트 및 검증

### 9.1 단위 테스트
```python
def test_clip_search():
    """CLIP 검색 테스트"""
    clip = CLIPSearch(model_id="openai/clip-vit-large-patch14")
    clip.build_index("data/ok_front/")
    results = clip.search_with_index("data/def_front/def_001.jpg", top_k=3)
    assert len(results) == 3
    assert 0 <= results[0].similarity <= 1

def test_roi_extraction():
    """ROI 추출 테스트"""
    img1 = Image.open("data/ok_front/ok_001.jpg")
    img2 = Image.open("data/def_front/def_001.jpg")
    rois, stats = extract_rois(img1, img2, topk=5)
    assert len(rois) <= 5
    assert stats["roi_count"] == len(rois)

def test_rag_search():
    """RAG 검색 테스트"""
    vectorstore = FAISS.load_local("manual_vectorstore", embeddings)
    docs = vectorstore.similarity_search("표면 변색 대응", k=3)
    assert len(docs) == 3
```

### 9.2 통합 테스트
```python
def test_end_to_end():
    """전체 파이프라인 테스트"""
    system = SimilarImageInspectionSystem()
    result = system.process("data/def_front/def_001.jpg", top_k=3)
    
    assert "main_image" in result
    assert "response" in result
    assert len(result["thumbnails"]) == 2  # TOP-3에서 메인 제외
```

---

## 10. 배포 및 운영

### 10.1 Docker 컨테이너화
```dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# Python 설치
RUN apt-get update && apt-get install -y python3.10 python3-pip

# 의존성 설치
COPY requirements.txt .
RUN pip3 install -r requirements.txt

# 앱 복사
COPY . /app
WORKDIR /app

# 벡터 DB 사전 구축
RUN python3 scripts/build_manual_vectorstore.py

# Gradio 실행
CMD ["python3", "app.py"]
```

### 10.2 API 서버 (FastAPI)
```python
from fastapi import FastAPI, UploadFile
from fastapi.responses import JSONResponse

app = FastAPI()
system = SimilarImageInspectionSystem()

@app.post("/api/inspect")
async def inspect_image(file: UploadFile, top_k: int = 3):
    """이미지 검사 API"""
    # 임시 저장
    temp_path = f"temp/{file.filename}"
    with open(temp_path, "wb") as f:
        f.write(await file.read())
    
    # 처리
    result = system.process(temp_path, top_k)
    
    return JSONResponse(content=result)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 11. 향후 개선 계획

### 11.1 단기 (1-2개월)
- [ ] Gradio UI 구현 및 기본 기능 테스트
- [ ] CLIP 검색 정확도 평가
- [ ] Anomaly Detection vs YOLO 성능 비교
- [ ] RAG 매뉴얼 검색 정확도 평가

### 11.2 중기 (3-6개월)
- [ ] YOLO 세그멘테이션 파인튜닝 (도메인 특화)
- [ ] LLM 파인튜닝 (불량 분석 태스크)
- [ ] 실시간 처리 최적화
- [ ] 다중 카메라 뷰 지원

### 11.3 장기 (6개월 이상)
- [ ] 자동 학습 파이프라인 (Active Learning)
- [ ] 불량 예측 모델 추가
- [ ] 모바일 앱 연동
- [ ] 생산 라인 통합

---

## 12. 참고 자료

### 12.1 모델 및 라이브러리
- **CLIP**: https://github.com/openai/CLIP
- **LLaVA**: https://llava-vl.github.io/
- **EXAONE**: https://huggingface.co/LGAI-EXAONE
- **LangChain**: https://python.langchain.com/
- **FAISS**: https://github.com/facebookresearch/faiss
- **Ultralytics YOLO**: https://docs.ultralytics.com/

### 12.2 한국어 임베딩 모델
- **jhgan/ko-sbert-nli**: https://huggingface.co/jhgan/ko-sbert-nli
- **sentence-transformers/paraphrase-multilingual-mpnet-base-v2**

### 12.3 기존 코드베이스
- GitHub: https://github.com/scschwan/llm_chal_vlm.git

---

## 13. 부록: 코드 개선 제안

### 13.1 modules/clip_search.py 개선
현재 코드는 잘 작동하지만, 다음 기능 추가를 권장합니다:

```python
class CLIPSearch:
    # ... 기존 코드 ...
    
    def save_index(self, save_path: str):
        """인덱스 저장 (재사용)"""
        if self.faiss_index is not None:
            faiss.write_index(self.faiss_index, f"{save_path}/index.faiss")
        torch.save({
            "gallery_paths": self.gallery_paths,
            "gallery_embs": self.gallery_embs
        }, f"{save_path}/data.pt")
    
    def load_index(self, load_path: str):
        """저장된 인덱스 로드"""
        data = torch.load(f"{load_path}/data.pt")
        self.gallery_paths = data["gallery_paths"]
        self.gallery_embs = data["gallery_embs"]
        
        if _HAS_FAISS:
            self.faiss_index = faiss.read_index(f"{load_path}/index.faiss")
```

### 13.2 modules/region_detect.py 개선
버그 수정: `boxes_to_prompt_hints()` 함수에서 조기 리턴 문제

```python
# 기존 (버그)
def boxes_to_prompt_hints(boxes: List[Box], tag: str, total_px: int) -> List[str]:
    hints: List[str] = []
    for i, b in enumerate(boxes, 1):
        pct = 100.0 * (b.area() / float(total_px))
        hints.append(f"{tag}{i}: x={b.x}, y={b.y}, w={b.w}, h={b.h}, area={b.area()}px (~{pct:.2f}%)")
        return hints  # ❌ 첫 번째 박스만 반환하고 종료

# 수정
def boxes_to_prompt_hints(boxes: List[Box], tag: str, total_px: int) -> List[str]:
    hints: List[str] = []
    for i, b in enumerate(boxes, 1):
        pct = 100.0 * (b.area() / float(total_px))
        hints.append(f"{tag}{i}: x={b.x}, y={b.y}, w={b.w}, h={b.h}, area={b.area()}px (~{pct:.2f}%)")
    return hints  # ✅ 모든 박스 반환
```

### 13.3 modules/vlm_local.py 개선
현재는 플레이스홀더이므로, 실제 LLaVA 구현 추가:

```python
from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration

class VLM:
    def __init__(self, model_id, device="cuda", use_bf16=True, ...):
        self.processor = LlavaNextProcessor.from_pretrained(model_id)
        self.model = LlavaNextForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,
            device_map=device
        )
    
    def compare_regions_text(self, left_path, right_path, prompt, ...):
        # 이미지 로드
        images = [Image.open(left_path), Image.open(right_path)]
        
        # 프롬프트 구성
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "image"},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        
        # 프롬프트 템플릿 적용
        text_prompt = self.processor.apply_chat_template(
            conversation, 
            add_generation_prompt=True
        )
        
        # 입력 준비
        inputs = self.processor(
            text=text_prompt,
            images=images,
            return_tensors="pt"
        ).to(self.model.device)
        
        # 생성
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=repetition_penalty
        )
        
        # 디코딩
        response = self.processor.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return response
```

---

## 14. 결론

본 설계서는 기존 `llm_chal_vlm` 코드베이스를 기반으로 **CLIP 유사이미지 검색 → Anomaly Detection 차이 분석 → RAG 기반 LLM 대응 매뉴얼 생성**의 3단계 파이프라인을 구현하기 위한 종합적인 가이드입니다.

주요 특징:
- ✅ **기존 코드 최대 활용**: CLIP 검색, ROI 검출 등 이미 구현된 모듈 재사용
- ✅ **모듈화 설계**: 각 기능을 독립적으로 개발/테스트 가능
- ✅ **확장 가능성**: YOLO, RAG 등 추가 기능 통합 용이
- ✅ **사용자 친화적 UI**: Gradio 기반 직관적 인터페이스

이 설계서를 바탕으로 단계별로 구현을 진행하시면 됩니다. 추가 질문이나 상세 구현이 필요하면 언제든지 요청해 주세요!
