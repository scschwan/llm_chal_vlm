# modules/vlm_local.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Optional, Callable, Dict, Any
import json
import torch
from PIL import Image

# ì‹¤ì œ í™˜ê²½ì—ì„œëŠ”:
#  - LlavaNextProcessor.from_pretrained(self.model_id)
#  - LlavaNextForConditionalGeneration.from_pretrained(... torch_dtype=...)
#  - model.to(self.device)
# ë“±ìœ¼ë¡œ ì—°ê²°í•˜ë©´ ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì‹¤í–‰ ë§‰íˆì§€ ì•Šë„ë¡ ìŠ¤í…ì„ ì œê³µí•©ë‹ˆë‹¤.

@dataclass
class VLM:
    model_id: str
    device: str = "cuda"
    persist: bool = False
    use_bf16: bool = True
    max_edge: int = 640
    verbose: bool = True

    def __post_init__(self):
        if self.verbose:
            print("ğŸ§° processor: LlavaNextProcessor (placeholder)")
            print("âœ… LlavaNextForConditionalGeneration ë¡œë“œ")
            print(f"ğŸ“¦ model.to({self.device}) ì™„ë£Œ")

    def _prepare_image_tensor(self, pil_img: Image.Image, max_edge_override: Optional[int] = None):
        # ì‹¤ì œëŠ” processor(images=..., return_tensors="pt") ë“±ì˜ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰
        # ì—¬ê¸°ì„œëŠ” shapeë§Œ ë§ì¶˜ ë”ë¯¸ í…ì„œë¡œ ëŒ€ì²´
        arr = torch.randn(1, 3, 224, 224)
        return arr.to(self.device)

    def compare_regions_text(
        self,
        left_path: str,
        right_path: str,
        prompt: str,
        max_new_tokens: int = 360,
        do_sample: bool = False,
        temperature: float = 0.0,
        top_p: float = 1.0,
        repetition_penalty: float = 1.1,
        preprocess: Optional[Callable[[Image.Image], Image.Image]] = None,
        max_edge_override: Optional[int] = None,
    ) -> str:
        """
        ì¢Œ/ìš° ì´ë¯¸ì§€ ë‘ ì¥ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ ë¹„êµ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±.
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” LLaVA chat-like inferenceë¥¼ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤.
        ì—¬ê¸°ì„œëŠ” ì‹¤í–‰ ë§‰í˜ ë°©ì§€ë¥¼ ìœ„í•´ ê³ ì •ëœ í¬ë§·ì˜ ë”ë¯¸ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.
        """
        left_img  = Image.open(left_path).convert("RGB")
        right_img = Image.open(right_path).convert("RGB")
        if preprocess:
            left_img  = preprocess(left_img)
            right_img = preprocess(right_img)

        # ì‹¤ì œ êµ¬í˜„: processor.apply_chat_template + model.generate(...)
        # í˜„ì¬ëŠ” placeholder
        pseudo_answer = (
            "[INFO] ë¶„ì„ ëŒ€ìƒ í•œ ì¤„ ì•Œë¦¼( 'ê¸ˆì† ê°€ê³µ ë¶€í’ˆ 1EA, ì¢Œ=ì •ìƒ, ìš°=í›„ë³´')\n"
            "[SCENE] ë‘ ì´ë¯¸ì§€ëŠ” íšŒìƒ‰ ë°°ê²½ ìœ„ ê¸ˆì† ê°€ê³µ ë¶€í’ˆì´ ë†“ì—¬ ìˆìœ¼ë©° ìœ ì‚¬í•œ ê°ë„ì—ì„œ ì´¬ì˜ë¨.\n"
            "[DETAIL] ì¢Œ/ìš° íŠ¹ì§• ë¶ˆë¦¿ ... (ì—¬ê¸°ì— ìœ„ì¹˜Â·í˜•íƒœÂ·ê°•ë„ ê¸°ë°˜ bulletë“¤ì´ ì™€ì•¼ í•¨)\n"
            "[ì¶”ë¡ ] ì¢Œì¸¡ì€ ê¸°ì¤€ í˜•íƒœë¡œ ë³´ì´ê³ , ìš°ì¸¡ì€ íŠ¹ì • ì˜ì—­ì—ì„œ êµ­ë¶€ í•¨ëª°(ê°•ë„ 3) ë“± ì°¨ì´ê°€ ê´€ì°°ë˜ë©° ë¶ˆëŸ‰ ì˜ì‹¬.\n"
            "[STATUS] ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ."
        )
        return pseudo_answer

    # ====== (ì˜µì…˜) í”„ë¡¬í”„íŠ¸ ì˜í–¥ ìµœì†Œí™”ë¥¼ ìœ„í•œ ì¤‘ë¦½ JSON ìº¡ì…˜ API ======
    def describe_image_json(
        self,
        image_path: str,
        *,
        schema: Dict[str, Any],
        max_new_tokens: int = 256,
        do_sample: bool = False,
    ) -> str:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ê´€ì°°í•´ schema í˜•íƒœì˜ JSONë§Œ ì¶œë ¥í•˜ëŠ” API (ìŠ¤í…).
        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë‹¨ì¼ ì´ë¯¸ì§€ ìº¡ì…˜ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•´ generate í›„ JSONë§Œ íŒŒì‹±.
        ì—¬ê¸°ì„œëŠ” ë¹ˆ ìŠ¤í‚¤ë§ˆë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í•´ ì‹¤í–‰ì´ ë©ˆì¶”ì§€ ì•Šë„ë¡ í•¨.
        """
        # ì‹¤ì œ êµ¬í˜„ ì‹œì—ëŠ” image_pathë¥¼ ì—´ê³  processor/ëª¨ë¸ë¡œ inference í›„ JSON ìƒì„±
        # ì§€ê¸ˆì€ ì…ë ¥ schemaì˜ í‚¤ë§Œ ìœ ì§€í•˜ëŠ” ë¹ˆ JSON ê°ì²´ë¥¼ ë°˜í™˜
        out = {k: ([] if isinstance(v, list) else ({} if isinstance(v, dict) else v))
               for k, v in schema.items()}
        return json.dumps(out, ensure_ascii=False)
