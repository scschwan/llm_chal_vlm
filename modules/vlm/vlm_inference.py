"""
VLM (Vision Language Model) ì¶”ë¡  ì—”ì§„
ë©€í‹°ëª¨ë‹¬ ì´ë¯¸ì§€ ë¶„ì„
"""

from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Optional, Union
import torch
from PIL import Image
from transformers import (
    LlavaNextProcessor,
    LlavaNextForConditionalGeneration,
    BitsAndBytesConfig
)


class VLMInference:
    """VLM ì¶”ë¡  ì—”ì§„"""
    
    def __init__(
        self,
        model_name: str = "llava-hf/llava-v1.6-mistral-7b-hf",
        device: str = "cuda",
        use_4bit: bool = False,
        use_8bit: bool = False,
        verbose: bool = True
    ):
        """
        Args:
            model_name: HuggingFace ëª¨ë¸ëª…
                - "llava-hf/llava-v1.6-mistral-7b-hf" (ì¶”ì²œ)
                - "LGAI-EXAONE/EXAONE-3.5-VL" (í•œêµ­ì–´ íŠ¹í™”, ë¯¸ì§€ì› ì‹œ)
            device: ë””ë°”ì´ìŠ¤ (cuda/cpu)
            use_4bit: 4-bit ì–‘ìí™” ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
            use_8bit: 8-bit ì–‘ìí™” ì‚¬ìš©
            verbose: ë¡œê·¸ ì¶œë ¥
        """
        self.model_name = model_name
        self.device = device
        self.verbose = verbose
        
        if self.verbose:
            print(f"ğŸ¤– VLM ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        
        # ì–‘ìí™” ì„¤ì •
        quantization_config = None
        if use_4bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            if self.verbose:
                print("âš™ï¸  4-bit ì–‘ìí™” í™œì„±í™”")
        elif use_8bit:
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True
            )
            if self.verbose:
                print("âš™ï¸  8-bit ì–‘ìí™” í™œì„±í™”")
        
        # Processor ë¡œë“œ
        self.processor = LlavaNextProcessor.from_pretrained(model_name)
        
        # ëª¨ë¸ ë¡œë“œ
        model_kwargs = {
            "torch_dtype": torch.float16,
            "device_map": device
        }
        
        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config
        
        self.model = LlavaNextForConditionalGeneration.from_pretrained(
            model_name,
            **model_kwargs
        )
        
        if self.verbose:
            print("âœ… VLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def analyze_defect_with_segmentation(
        self,
        normal_image_path: Union[str, Path],
        defect_image_path: Union[str, Path],
        overlay_image_path: Union[str, Path],
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        do_sample: bool = True
    ) -> str:
        """
        3ê°œ ì´ë¯¸ì§€ ê¸°ë°˜ ë¶ˆëŸ‰ ë¶„ì„
        
        Args:
            normal_image_path: ì •ìƒ ê¸°ì¤€ ì´ë¯¸ì§€
            defect_image_path: ë¶ˆëŸ‰ ì˜ì‹¬ ì´ë¯¸ì§€
            overlay_image_path: ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€
            prompt: ë¶„ì„ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            do_sample: ìƒ˜í”Œë§ ì—¬ë¶€
        
        Returns:
            VLM ë¶„ì„ ê²°ê³¼ í…ìŠ¤íŠ¸
        """
        if self.verbose:
            print("ğŸ–¼ï¸  ì´ë¯¸ì§€ ë¡œë“œ ì¤‘...")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        images = [
            Image.open(normal_image_path).convert("RGB"),
            Image.open(defect_image_path).convert("RGB"),
            Image.open(overlay_image_path).convert("RGB")
        ]
        
        if self.verbose:
            print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            print("ğŸ”® VLM ì¶”ë¡  ì¤‘...")
        
        # ëŒ€í™” í˜•ì‹ êµ¬ì„± (LLaVA NextëŠ” chat template ì‚¬ìš©)
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€
                    {"type": "image"},  # ë‘ ë²ˆì§¸ ì´ë¯¸ì§€
                    {"type": "image"},  # ì„¸ ë²ˆì§¸ ì´ë¯¸ì§€
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
        text_prompt = self.processor.apply_chat_template(
            conversation,
            add_generation_prompt=True
        )
        
        # ì…ë ¥ ì¤€ë¹„
        inputs = self.processor(
            text=text_prompt,
            images=images,
            return_tensors="pt"
        ).to(self.device)
        
        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_p=0.9,
                repetition_penalty=1.1
            )
        
        # ë””ì½”ë”© (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸)
        generated_text = self.processor.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        if self.verbose:
            print(f"âœ… VLM ë¶„ì„ ì™„ë£Œ ({len(generated_text)} ë¬¸ì)")
        
        return generated_text.strip()
    
    def analyze_simple(
        self,
        image_paths: List[Union[str, Path]],
        prompt: str,
        max_new_tokens: int = 512
    ) -> str:
        """
        ê°„ë‹¨í•œ ë©€í‹° ì´ë¯¸ì§€ ë¶„ì„ (ìœ ì—°í•œ ì´ë¯¸ì§€ ê°œìˆ˜)
        
        Args:
            image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            prompt: ë¶„ì„ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        
        Returns:
            VLM ë¶„ì„ ê²°ê³¼
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        images = [Image.open(p).convert("RGB") for p in image_paths]
        
        # ëŒ€í™” êµ¬ì„±
        content = [{"type": "image"} for _ in images]
        content.append({"type": "text", "text": prompt})
        
        conversation = [{"role": "user", "content": content}]
        
        # í”„ë¡¬í”„íŠ¸ ì ìš©
        text_prompt = self.processor.apply_chat_template(
            conversation,
            add_generation_prompt=True
        )
        
        # ì¶”ë¡ 
        inputs = self.processor(
            text=text_prompt,
            images=images,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                do_sample=True
            )
        
        generated_text = self.processor.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í™•ë³´)"""
        if self.verbose:
            print("ğŸ—‘ï¸  VLM ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
        
        del self.model
        del self.processor
        torch.cuda.empty_cache()
        
        if self.verbose:
            print("âœ… VLM ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")


if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    print("VLM ì¶”ë¡  ì—”ì§„ í…ŒìŠ¤íŠ¸")
    print("ì‹¤ì œ ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    print("\nì‚¬ìš© ì˜ˆì‹œ:")
    print("""
        vlm = VLMInference(
            model_name="llava-hf/llava-v1.6-mistral-7b-hf",
            use_4bit=True,  # ë©”ëª¨ë¦¬ ì ˆì•½
            verbose=True
        )

        result = vlm.analyze_defect_with_segmentation(
            normal_image_path="normal.jpg",
            defect_image_path="defect.jpg",
            overlay_image_path="overlay.jpg",
            prompt="ë¶ˆëŸ‰ì„ ë¶„ì„í•˜ì„¸ìš”..."
        )

        print(result)
    """)