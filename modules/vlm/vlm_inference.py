"""
VLM (Vision Language Model) ì¶”ë¡  ì—”ì§„
ë©€í‹°ëª¨ë‹¬ ì´ë¯¸ì§€ ë¶„ì„
"""

from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Optional, Union
import torch
from PIL import Image

try:
    from transformers import (
        LlavaNextProcessor,
        LlavaNextForConditionalGeneration,
        BitsAndBytesConfig
    )
    LLAVA_AVAILABLE = True
except ImportError:
    # LlavaNextë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ëŒ€ì²´
    try:
        from transformers import (
            AutoProcessor,
            LlavaForConditionalGeneration as LlavaNextForConditionalGeneration,
            BitsAndBytesConfig
        )
        LlavaNextProcessor = AutoProcessor
        LLAVA_AVAILABLE = True
    except ImportError:
        LLAVA_AVAILABLE = False
        print("âš ï¸ Transformers LLaVA ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")



class VLMInference:
    """VLM ì¶”ë¡  ì—”ì§„"""
    
    def __init__(
        self,
        model_name: str = "llava-hf/llava-v1.6-mistral-7b-hf",
        device: str = "cuda",
        use_4bit: bool = False,
        use_8bit: bool = False,
        verbose: bool = True
    ):
        if not LLAVA_AVAILABLE:
            raise ImportError(
                "Transformersì˜ LLaVA ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                "transformers ë²„ì „ì„ 4.37.0 ì´ìƒìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ì„¸ìš”: "
                "pip install transformers>=4.37.0 --upgrade"
            )
        
        self.model_name = model_name
        self.device = device
        self.verbose = verbose
        
        if self.verbose:
            print(f"ğŸ¤– VLM ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        
        # ì–‘ìí™” ì„¤ì •
        quantization_config = None
        if use_4bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            if self.verbose:
                print("âš™ï¸  4-bit ì–‘ìí™” í™œì„±í™”")
        elif use_8bit:
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True
            )
            if self.verbose:
                print("âš™ï¸  8-bit ì–‘ìí™” í™œì„±í™”")
        
        # âœ… Processor ë¡œë“œ - try-except ì¶”ê°€
        try:
            self.processor = LlavaNextProcessor.from_pretrained(model_name)
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸  LlavaNextProcessor ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("   AutoProcessorë¡œ ì¬ì‹œë„...")
            
            # í´ë°±: AutoProcessor ì‚¬ìš©
            from transformers import AutoProcessor
            try:
                self.processor = AutoProcessor.from_pretrained(model_name)
            except Exception as e2:
                if self.verbose:
                    print(f"âš ï¸  AutoProcessorë„ ì‹¤íŒ¨: {e2}")
                
                # ë§ˆì§€ë§‰ ì‹œë„: LlavaProcessor
                try:
                    from transformers import LlavaProcessor
                    self.processor = LlavaProcessor.from_pretrained(model_name)
                    if self.verbose:
                        print("âœ… LlavaProcessorë¡œ ëŒ€ì²´ ì„±ê³µ")
                except Exception as e3:
                    raise ImportError(
                        f"Processor ë¡œë“œ ì‹¤íŒ¨. ëª¨ë“  ì‹œë„ ì‹¤íŒ¨:\n"
                        f"1. LlavaNextProcessor: {e}\n"
                        f"2. AutoProcessor: {e2}\n"
                        f"3. LlavaProcessor: {e3}"
                    )
        
        # ëª¨ë¸ ë¡œë“œ
        model_kwargs = {
            "torch_dtype": torch.float16,
            "device_map": device
        }
        
        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config
        
        # âœ… ëª¨ë¸ ë¡œë“œë„ try-except ì¶”ê°€
        try:
            self.model = LlavaNextForConditionalGeneration.from_pretrained(
                model_name,
                **model_kwargs
            )
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸  LlavaNextForConditionalGeneration ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("   LlavaForConditionalGenerationìœ¼ë¡œ ì¬ì‹œë„...")
            
            from transformers import LlavaForConditionalGeneration
            self.model = LlavaForConditionalGeneration.from_pretrained(
                model_name,
                **model_kwargs
            )
        
        if self.verbose:
            print("âœ… VLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def analyze_defect_with_segmentation(
        self,
        normal_image_path: Union[str, Path],
        defect_image_path: Union[str, Path],
        overlay_image_path: Union[str, Path],
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        do_sample: bool = True
    ) -> str:
        """
        3ê°œ ì´ë¯¸ì§€ ê¸°ë°˜ ë¶ˆëŸ‰ ë¶„ì„
        """
        if self.verbose:
            print("ğŸ–¼ï¸  ì´ë¯¸ì§€ ë¡œë“œ ì¤‘...")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        images = [
            Image.open(normal_image_path).convert("RGB"),
            Image.open(defect_image_path).convert("RGB"),
            Image.open(overlay_image_path).convert("RGB")
        ]
        
        if self.verbose:
            print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            print("ğŸ”® VLM ì¶”ë¡  ì¤‘...")
        
        # ëŒ€í™” í˜•ì‹ êµ¬ì„±
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "image"},
                    {"type": "image"},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        
        # âœ… ìˆ˜ì •: try-exceptë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        try:
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
            text_prompt = self.processor.apply_chat_template(
                conversation,
                add_generation_prompt=True
            )
        except TypeError as e:
            # apply_chat_templateì´ ì‹¤íŒ¨í•˜ë©´ ì§ì ‘ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            if self.verbose:
                print(f"âš ï¸  Chat template ì ìš© ì‹¤íŒ¨, ì§ì ‘ í”„ë¡¬í”„íŠ¸ êµ¬ì„±: {e}")
            
            # LLaVA ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í˜•ì‹
            text_prompt = f"USER: <image><image><image>\n{prompt}\nASSISTANT:"
        
        # ì…ë ¥ ì¤€ë¹„
        try:
            inputs = self.processor(
                text=text_prompt,
                images=images,
                return_tensors="pt",
                padding=True
            ).to(self.device)
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸  Processor ì˜¤ë¥˜: {e}")
                print("   ê¸°ë³¸ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
            
            # í´ë°±: ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë”°ë¡œ ì²˜ë¦¬
            inputs = self.processor(
                images=images,
                text=text_prompt,
                return_tensors="pt",
                padding=True
            ).to(self.device)
        
        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.processor.tokenizer.pad_token_id if hasattr(self.processor, 'tokenizer') else None
            )
        
        # ë””ì½”ë”©
        try:
            generated_text = self.processor.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )
        except:
            # ì „ì²´ ë””ì½”ë”©
            generated_text = self.processor.decode(
                outputs[0],
                skip_special_tokens=True
            )
        
        if self.verbose:
            print(f"âœ… VLM ë¶„ì„ ì™„ë£Œ ({len(generated_text)} ë¬¸ì)")
        
        return generated_text.strip()
    
    def analyze_simple(
        self,
        image_paths: List[Union[str, Path]],
        prompt: str,
        max_new_tokens: int = 512
    ) -> str:
        """
        ê°„ë‹¨í•œ ë©€í‹° ì´ë¯¸ì§€ ë¶„ì„ (ìœ ì—°í•œ ì´ë¯¸ì§€ ê°œìˆ˜)
        
        Args:
            image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            prompt: ë¶„ì„ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        
        Returns:
            VLM ë¶„ì„ ê²°ê³¼
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        images = [Image.open(p).convert("RGB") for p in image_paths]
        
        # ëŒ€í™” êµ¬ì„±
        content = [{"type": "image"} for _ in images]
        content.append({"type": "text", "text": prompt})
        
        conversation = [{"role": "user", "content": content}]
        
        # í”„ë¡¬í”„íŠ¸ ì ìš©
        text_prompt = self.processor.apply_chat_template(
            conversation,
            add_generation_prompt=True
        )
        
        # ì¶”ë¡ 
        inputs = self.processor(
            text=text_prompt,
            images=images,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                do_sample=True
            )
        
        generated_text = self.processor.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í™•ë³´)"""
        if self.verbose:
            print("ğŸ—‘ï¸  VLM ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
        
        del self.model
        del self.processor
        torch.cuda.empty_cache()
        
        if self.verbose:
            print("âœ… VLM ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")


if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    print("VLM ì¶”ë¡  ì—”ì§„ í…ŒìŠ¤íŠ¸")
    print("ì‹¤ì œ ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    print("\nì‚¬ìš© ì˜ˆì‹œ:")
    print("""
        vlm = VLMInference(
            model_name="llava-hf/llava-v1.6-mistral-7b-hf",
            use_4bit=True,  # ë©”ëª¨ë¦¬ ì ˆì•½
            verbose=True
        )

        result = vlm.analyze_defect_with_segmentation(
            normal_image_path="normal.jpg",
            defect_image_path="defect.jpg",
            overlay_image_path="overlay.jpg",
            prompt="ë¶ˆëŸ‰ì„ ë¶„ì„í•˜ì„¸ìš”..."
        )

        print(result)
    """)