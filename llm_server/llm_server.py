# llm_server.py
"""
LLM ì „ìš© API ì„œë²„
ë³„ë„ ê°€ìƒí™˜ê²½(venv_llm)ì—ì„œ ì‹¤í–‰
tokenizers 0.15.2 ì‚¬ìš©
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Optional
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import uvicorn

app = FastAPI(title="LLM Server", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ëª¨ë¸
model = None
tokenizer = None
model_name = None


class AnalysisRequest(BaseModel):
    """ë¶ˆëŸ‰ ë¶„ì„ ìš”ì²­"""
    product: str
    defect_en: str
    defect_ko: str
    full_name_ko: str
    anomaly_score: float
    is_anomaly: bool
    manual_context: Dict[str, List[str]]
    max_new_tokens: int = 512
    temperature: float = 0.7


class AnalysisResponse(BaseModel):
    """ë¶ˆëŸ‰ ë¶„ì„ ì‘ë‹µ"""
    status: str
    analysis: str
    model: str


@app.on_event("startup")
async def load_model():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global model, tokenizer, model_name
    
    # í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’
    model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"
    
    print("=" * 60)
    print(f"ğŸ¤– LLM ì„œë²„ ì‹œì‘ ì¤‘...")
    print(f"ğŸ“¦ ëª¨ë¸: {model_name}")
    print("=" * 60)
    
    try:
        # ì–‘ìí™” ì„¤ì •
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        print("ğŸ”„ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
        try :
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                use_fast=True,
                local_files_only=False,
                force_download=True,   # ìºì‹œê°€ ì´ìƒí•˜ë©´ ìƒˆë¡œ ë°›ê¸°
            )
            print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
        except Exception as e :
            print(f"[WARN] Fast tokenizer failed: {e}\n--> Falling back to slow tokenizer.")
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                use_fast=False,
                local_files_only=False,
                force_download=True,
            )
            print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ (slow)")
        
        print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘ (4-bit ì–‘ìí™”)...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            trust_remote_code=True,
            device_map="auto"
        )
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        print("=" * 60)
        print("âœ… LLM ì„œë²„ ì¤€ë¹„ ì™„ë£Œ")
        print(f"ğŸŒ í¬íŠ¸: 5001")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


@app.post("/analyze", response_model=AnalysisResponse)
async def analyze(request: AnalysisRequest):
    """ë¶ˆëŸ‰ ë¶„ì„ ìˆ˜í–‰"""
    
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = _build_prompt(request)
        
        # HyperCLOVA-X í˜•ì‹ìœ¼ë¡œ ìƒì„±
        chat = [
            {"role": "tool_list", "content": ""},
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì œì¡°ì—… í’ˆì§ˆ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
        
        inputs = tokenizer.apply_chat_template(
            chat,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        inputs = inputs.to(model.device)
        
        # ìƒì„±
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_length=request.max_new_tokens + inputs["input_ids"].shape[1],
                stop_strings=["<|endofturn|>", "<|stop|>"],
                tokenizer=tokenizer,
                temperature=request.temperature,
                do_sample=True if request.temperature > 0 else False
            )
        
        # ë””ì½”ë”©
        generated_text = tokenizer.decode(
            output_ids[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return AnalysisResponse(
            status="success",
            analysis=generated_text.strip(),
            model=model_name
        )
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def _build_prompt(request: AnalysisRequest) -> str:
    """ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    causes = "\n".join([f"- {c}" for c in request.manual_context.get("ì›ì¸", [])])
    actions = "\n".join([f"- {a}" for a in request.manual_context.get("ì¡°ì¹˜", [])])
    
    prompt = f"""ë‹¹ì‹ ì€ ì œì¡°ì—… í’ˆì§ˆ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¶ˆëŸ‰ ì •ë³´ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

## ë¶ˆëŸ‰ ì •ë³´
- ì œí’ˆ: {request.product}
- ë¶ˆëŸ‰ ìœ í˜•: {request.defect_ko} ({request.defect_en})
- ì •ì‹ ëª…ì¹­: {request.full_name_ko}
- ì´ìƒ ê²€ì¶œ ì ìˆ˜: {request.anomaly_score:.4f}
- ë¶ˆëŸ‰ íŒì •: {"ë¶ˆëŸ‰" if request.is_anomaly else "ì •ìƒ"}

## ë§¤ë‰´ì–¼ ì°¸ì¡°

### ë°œìƒ ì›ì¸
{causes if causes else "ë§¤ë‰´ì–¼ ì •ë³´ ì—†ìŒ"}

### ì¡°ì¹˜ ê°€ì´ë“œ
{actions if actions else "ë§¤ë‰´ì–¼ ì •ë³´ ì—†ìŒ"}

## ì‘ì„± ìš”ì²­
ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•œ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”:

1. **ë¶ˆëŸ‰ í˜„í™© ìš”ì•½**: ê²€ì¶œëœ ë¶ˆëŸ‰ì˜ íŠ¹ì§•ê³¼ ì‹¬ê°ë„
2. **ì›ì¸ ë¶„ì„**: ë§¤ë‰´ì–¼ì„ ì°¸ê³ í•œ ë°œìƒ ì›ì¸
3. **ëŒ€ì‘ ë°©ì•ˆ**: êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì¹˜ ë°©ë²•
4. **ì˜ˆë°© ì¡°ì¹˜**: ì¬ë°œ ë°©ì§€ ê¶Œì¥ì‚¬í•­

í˜„ì¥ì—ì„œ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
    
    return prompt


@app.get("/health")
async def health():
    """í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "model": model_name,
        "model_loaded": model is not None
    }


@app.get("/")
async def root():
    """ë£¨íŠ¸"""
    return {
        "service": "LLM Server",
        "model": model_name,
        "endpoints": ["/analyze", "/health"]
    }


if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=5001,
        log_level="info"
    )