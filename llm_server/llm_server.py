# llm_server.py  (LLM + VLM ë™ì‹œ ì§€ì› ë²„ì „)

import os
import time
from typing import Dict, List, Optional

import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from PIL import Image

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoProcessor,
    LlavaForConditionalGeneration,
)

# =========================
# FastAPI
# =========================
app = FastAPI(title="LLM/VLM Server", version="1.0")

# =========================
# ì „ì—­ ëª¨ë¸ í•¸ë“¤ (LLM)
# =========================
llm_name: Optional[str] = None
llm_model: Optional[AutoModelForCausalLM] = None
llm_tokenizer: Optional[AutoTokenizer] = None

# =========================
# ì „ì—­ ëª¨ë¸ í•¸ë“¤ (VLM - LLaVA)
# =========================
vlm_name: Optional[str] = None
vlm_model: Optional[LlavaForConditionalGeneration] = None
vlm_processor: Optional[AutoProcessor] = None

# =========================
# ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
# =========================
class AnalysisRequest(BaseModel):
    product: str
    defect_en: str
    defect_ko: str
    full_name_ko: str
    anomaly_score: float = 0.0
    is_anomaly: bool = False
    manual_context: Dict[str, List[str]] = {}
    max_new_tokens: int = 512
    temperature: float = 0.7

class VLMAnalysisRequest(BaseModel):
    image_path: str
    prompt: str
    max_new_tokens: int = 256
    temperature: float = 0.2

# =========================
# ìœ í‹¸: í”„ë¡¬í”„íŠ¸ ë¹Œë”(LLM)
# =========================
def _build_prompt(req: AnalysisRequest) -> str:
    """ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ë¹Œë”"""
    
    # ë§¤ë‰´ì–¼ ì •ë¦¬ (ì¤‘ë³µ ì œê±° ë° ê°„ê²°í™”)
    causes_list = req.manual_context.get("ì›ì¸", [])
    actions_list = req.manual_context.get("ì¡°ì¹˜", [])
    
    # í•´ë‹¹ ë¶ˆëŸ‰ë§Œ í•„í„°ë§ (defect_en ê¸°ì¤€)
    causes = []
    actions = []
    
    for cause_text in causes_list:
        # í˜„ìž¬ ë¶ˆëŸ‰(defect_en)ê³¼ ê´€ë ¨ëœ ë‚´ìš©ë§Œ ì¶”ì¶œ
        if req.defect_en.lower() in cause_text.lower() or req.defect_ko in cause_text:
            # ê¹”ë”í•˜ê²Œ ì •ë¦¬
            lines = [line.strip() for line in cause_text.split('\n') 
                    if line.strip() and not line.strip().startswith(('burr', 'hole', 'scratch', 'Hole', 'burr', 'Scratch'))]
            causes.extend(lines[:3])  # ìµœëŒ€ 3ì¤„
    
    for action_text in actions_list:
        if req.defect_en.lower() in action_text.lower() or req.defect_ko in action_text:
            lines = [line.strip() for line in action_text.split('\n') 
                    if line.strip() and not line.strip().startswith(('burr', 'hole', 'scratch', 'Hole', 'burr', 'Scratch'))]
            actions.extend(lines[:3])  # ìµœëŒ€ 3ì¤„
    
    # ë§¤ë‰´ì–¼ ì •ë³´ ìœ ë¬´ í™•ì¸
    has_manual = bool(causes or actions)
    
    # ì´ìƒ ê²€ì¶œ íŒì • ì„¤ëª…
    anomaly_status = "ë¶ˆëŸ‰" if req.is_anomaly else "ì •ìƒ"
    score_interpretation = ""
    if req.anomaly_score > 0.5:
        score_interpretation = "(ë†’ì€ ì´ìƒ ì ìˆ˜ - ëª…í™•í•œ ë¶ˆëŸ‰)"
    elif req.anomaly_score > 0.1:
        score_interpretation = "(ì¤‘ê°„ ì´ìƒ ì ìˆ˜ - ê²½ë¯¸í•œ ë¶ˆëŸ‰)"
    elif req.is_anomaly:
        score_interpretation = "(ë‚®ì€ ì´ìƒ ì ìˆ˜ - ê²½ê³„ì„ ìƒ)"
    else:
        score_interpretation = "(ì •ìƒ ë²”ìœ„ - ë¶ˆëŸ‰ ë¯¸ê²€ì¶œ)"
    
    prompt = f"""ë‹¹ì‹ ì€ ì œì¡°ì—… í’ˆì§ˆ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì•„ëž˜ ë¶ˆëŸ‰ ì •ë³´ë¥¼ **ê°„ê²°í•˜ê²Œ** ë¶„ì„í•˜ì„¸ìš”.

## ê²€ì‚¬ ì •ë³´
- ì œí’ˆ: {req.product}
- ë¶ˆëŸ‰ ìœ í˜•: {req.defect_ko} ({req.defect_en})
- ì •ì‹ ëª…ì¹­: {req.full_name_ko}
- ì´ìƒ ê²€ì¶œ ì ìˆ˜: {req.anomaly_score:.4f} {score_interpretation}
- ìµœì¢… íŒì •: {anomaly_status}

## ë§¤ë‰´ì–¼ ì •ë³´
"""
    
    if has_manual:
        if causes:
            prompt += "\n**ë°œìƒ ì›ì¸:**\n"
            for i, cause in enumerate(causes[:3], 1):
                prompt += f"{i}. {cause}\n"
        
        if actions:
            prompt += "\n**ì¡°ì¹˜ ê°€ì´ë“œ:**\n"
            for i, action in enumerate(actions[:3], 1):
                prompt += f"{i}. {action}\n"
    else:
        prompt += "- ë§¤ë‰´ì–¼ ì •ë³´ ì—†ìŒ (ì¼ë°˜ì ì¸ ì œì¡° ì§€ì‹ ê¸°ë°˜ ë¶„ì„ í•„ìš”)\n"
    
    prompt += f"""
## ìž‘ì„± ì§€ì¹¨
1. ìœ„ ë§¤ë‰´ì–¼ ì •ë³´ë¥¼ **ì§ì ‘ ì¸ìš©**í•˜ë©° ë¶„ì„
2. ë§¤ë‰´ì–¼ ë¬¸ìž¥ì€ "ë”°ì˜´í‘œ"ë¡œ í‘œì‹œ
3. **4ê°œ ì„¹ì…˜ë§Œ** ìž‘ì„±: [ë¶ˆëŸ‰ í˜„í™©] â†’ [ì›ì¸ ë¶„ì„] â†’ [ëŒ€ì‘ ë°©ì•ˆ] â†’ [ì˜ˆë°© ì¡°ì¹˜]
4. ê° ì„¹ì…˜ì€ **2-3ì¤„ë¡œ ê°„ê²°í•˜ê²Œ**
5. ì˜ˆì‹œë‚˜ í…œí”Œë¦¿ ë¬¸êµ¬ ë°˜ë³µ ê¸ˆì§€

## ì¶œë ¥ í˜•ì‹
### ë¶ˆëŸ‰ í˜„í™© ìš”ì•½
- (2-3ì¤„ ìš”ì•½)

### ì›ì¸ ë¶„ì„
- (ë§¤ë‰´ì–¼ ì¸ìš© + ë¶„ì„)

### ëŒ€ì‘ ë°©ì•ˆ
- (ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­ 2-3ê°œ)

### ì˜ˆë°© ì¡°ì¹˜
- (ìž¬ë°œ ë°©ì§€ ë°©ì•ˆ 2-3ê°œ)
"""
    
    return prompt
# =========================
# ëª¨ë¸ ë¡œë”
# =========================
@app.on_event("startup")
async def load_models_on_startup():
    global llm_name, llm_model, llm_tokenizer
    global vlm_name, vlm_model, vlm_processor

    # ---- LLM ----
    try:
        llm_name = os.getenv("LLM_MODEL", "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B")
        print(f"ðŸ”„ LLM ë¡œë“œ ì‹œë„: {llm_name}")

        # í† í¬ë‚˜ì´ì €: fast ìš°ì„ , ì‹¤íŒ¨ ì‹œ slow
        try:
            llm_tokenizer = AutoTokenizer.from_pretrained(
                llm_name, use_fast=True, trust_remote_code=True
            )
            print("âœ… LLM í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ (fast)")
        except Exception as e:
            print(f"[WARN] LLM fast tokenizer ì‹¤íŒ¨: {e} â†’ slow ìž¬ì‹œë„")
            llm_tokenizer = AutoTokenizer.from_pretrained(
                llm_name, use_fast=False, trust_remote_code=True
            )
            print("âœ… LLM í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ (slow)")

        llm_model = AutoModelForCausalLM.from_pretrained(
            llm_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            trust_remote_code=True,
        )
        print("âœ… LLM ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
        llm_name = None
        llm_model = None
        llm_tokenizer = None

    # ---- VLM (LLaVA) ----
    try:
        vlm_name = os.getenv("VLM_MODEL", "llava-hf/llava-1.5-7b-hf")
        print(f"ðŸ”„ VLM ë¡œë“œ ì‹œë„: {vlm_name}")

        vlm_model = LlavaForConditionalGeneration.from_pretrained(
            vlm_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
        )
        vlm_processor = AutoProcessor.from_pretrained(vlm_name)
        print("âœ… VLM ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ VLM ë¡œë“œ ê±´ë„ˆëœ€: {e}")
        vlm_name = None
        vlm_model = None
        vlm_processor = None

# =========================
# ë£¨íŠ¸/í—¬ìŠ¤
# =========================
@app.get("/")
def root():
    return {
        "service": "LLM/VLM Server",
        "models": {
            "llm": llm_name,
            "vlm": vlm_name,
        },
        "endpoints": ["/analyze", "/analyze_vlm", "/health"],
    }

@app.get("/health")
def health():
    return {
        "status": "healthy",
        "llm": {"name": llm_name, "loaded": llm_model is not None},
        "vlm": {"name": vlm_name, "loaded": vlm_model is not None},
    }

# =========================
# LLM ë¶„ì„
# =========================
@app.post("/analyze")
def analyze(req: AnalysisRequest):
    if llm_model is None or llm_tokenizer is None:
        raise HTTPException(503, detail="LLM not loaded")

    prompt = _build_prompt(req)

    # device ì¶”ì¶œ (device_map="auto"ì¼ ë•Œë„ ì²« íŒŒë¼ë¯¸í„°ì˜ device ì‚¬ìš©)
    try:
        device = next(llm_model.parameters()).device
    except StopIteration:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    inputs = llm_tokenizer(prompt, return_tensors="pt").to(device)

    do_sample = (req.temperature or 0) > 0
    gen_kwargs = dict(
        max_new_tokens=min(max(req.max_new_tokens, 16), 2048),
        temperature=float(max(min(req.temperature, 1.5), 0.0)),
        do_sample=do_sample,
        repetition_penalty=1.2,  # âœ… ì¶”ê°€: ë°˜ë³µ ë°©ì§€
    )
    if do_sample:
        gen_kwargs.update(dict(top_p=0.9))

    with torch.no_grad():
        output_ids = llm_model.generate(**inputs, **gen_kwargs)

    text = llm_tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return {
        "status": "success",
        "analysis": text,
        "model": llm_name,
        "used_temperature": gen_kwargs["temperature"],
        "max_new_tokens": gen_kwargs["max_new_tokens"],
    }

# =========================
# VLM ë¶„ì„ (LLaVA)
# =========================
@app.post("/analyze_vlm")
def analyze_vlm(req: VLMAnalysisRequest):
    if vlm_model is None or vlm_processor is None:
        raise HTTPException(503, detail="VLM not loaded")
    if not os.path.exists(req.image_path):
        raise HTTPException(400, detail=f"image_path not found: {req.image_path}")

    try:
        img = Image.open(req.image_path).convert("RGB")

        # ì‹ í˜• Processor: ì±„íŒ… í…œí”Œë¦¿(ë©€í‹°ëª¨ë‹¬) ì§€ì›
        try:
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": req.prompt.strip()},
                    ],
                }
            ]
            prompt_text = vlm_processor.apply_chat_template(
                messages, add_generation_prompt=True, tokenize=False
            )
        except Exception:
            # êµ¬í˜• í˜¸í™˜: í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ
            prompt_text = req.prompt.strip()

        inputs = vlm_processor(images=img, text=prompt_text, return_tensors="pt").to(vlm_model.device)

        do_sample = (req.temperature or 0) > 0
        gen_kwargs = dict(
            max_new_tokens=min(max(req.max_new_tokens, 16), 1024),
            temperature=float(max(min(req.temperature, 1.5), 0.0)),
            do_sample=do_sample,
        )
        if do_sample:
            gen_kwargs.update(dict(top_p=0.9))

        with torch.no_grad():
            out = vlm_model.generate(**inputs, **gen_kwargs)

        text = vlm_processor.batch_decode(out, skip_special_tokens=True)[0]
        return {
            "status": "success",
            "analysis": text,
            "model": vlm_name,
            "used_temperature": gen_kwargs["temperature"],
            "max_new_tokens": gen_kwargs["max_new_tokens"],
        }
    except HTTPException:
        raise
    except Exception as e:
        import traceback; traceback.print_exc()
        raise HTTPException(500, detail=f"VLM inference error: {e}")

# =========================
# ì„œë²„ ì‹¤í–‰ (ê°œë°œìš©)
# =========================
if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "5001"))
    uvicorn.run("llm_server:app", host="0.0.0.0", port=port, reload=False)
