# llm_server.py  (LLM + VLM ë™ì‹œ ì§€ì› ë²„ì „)

import os
import time
from typing import Dict, List, Optional

import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from PIL import Image

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoProcessor,
    LlavaForConditionalGeneration,
)

# =========================
# FastAPI
# =========================
app = FastAPI(title="LLM/VLM Server", version="1.0")

# =========================
# ì „ì—­ ëª¨ë¸ í•¸ë“¤ (LLM)
# =========================
llm_name: Optional[str] = None
llm_model: Optional[AutoModelForCausalLM] = None
llm_tokenizer: Optional[AutoTokenizer] = None

# =========================
# ì „ì—­ ëª¨ë¸ í•¸ë“¤ (VLM - LLaVA)
# =========================
vlm_name: Optional[str] = None
vlm_model: Optional[LlavaForConditionalGeneration] = None
vlm_processor: Optional[AutoProcessor] = None

# =========================
# ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
# =========================
class AnalysisRequest(BaseModel):
    product: str
    defect_en: str
    defect_ko: str
    full_name_ko: str
    anomaly_score: float = 0.0
    is_anomaly: bool = False
    manual_context: Dict[str, List[str]] = {}
    max_new_tokens: int = 512
    temperature: float = 0.7

class VLMAnalysisRequest(BaseModel):
    image_path: str
    prompt: str
    max_new_tokens: int = 256
    temperature: float = 0.2

# =========================
# ìœ í‹¸: í”„ë¡¬í”„íŠ¸ ë¹Œë”(LLM)
# =========================
def _build_prompt(req: AnalysisRequest) -> str:
    """ê¹”ë”í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    # ë§¤ë‰´ì–¼ ì •ë³´ (ì´ë¯¸ ì •ë¦¬ëœ ë¦¬ìŠ¤íŠ¸)
    causes = req.manual_context.get("ì›ì¸", [])
    actions = req.manual_context.get("ì¡°ì¹˜", [])
    
    has_manual = bool(causes or actions)
    
    # íŒì • ìƒíƒœ
    if req.is_anomaly:
        status = f"ë¶ˆëŸ‰ ê²€ì¶œ (ì´ìƒì ìˆ˜: {req.anomaly_score:.4f})"
    else:
        status = f"ì •ìƒ ë²”ìœ„ (ì´ìƒì ìˆ˜: {req.anomaly_score:.4f})"
    
    prompt = f"""ë‹¹ì‹ ì€ ì œì¡° í’ˆì§ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ã€ê²€ì‚¬ ê²°ê³¼ã€‘
ì œí’ˆ: {req.product}
ë¶ˆëŸ‰: {req.defect_ko} ({req.defect_en})
íŒì •: {status}

ã€ë§¤ë‰´ì–¼ã€‘
"""
    
    if has_manual:
        if causes:
            prompt += "ë°œìƒ ì›ì¸:\n"
            for i, cause in enumerate(causes, 1):
                prompt += f"{i}. {cause}\n"
        
        if actions:
            prompt += "\nì¡°ì¹˜ ë°©ë²•:\n"
            for i, action in enumerate(actions, 1):
                prompt += f"{i}. {action}\n"
    else:
        prompt += "â€» ë§¤ë‰´ì–¼ ì •ë³´ ì—†ìŒ\n"
    
    prompt += """
ã€ì§€ì¹¨ã€‘
- ìœ„ ë§¤ë‰´ì–¼ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš© (ë”°ì˜´í‘œ ì‚¬ìš©)
- 4ê°œ ì„¹ì…˜ë§Œ ì‘ì„± (ê° 2-3ë¬¸ì¥)
- ì¶”ì¸¡ì´ë‚˜ ì˜ˆì‹œ ë°˜ë³µ ê¸ˆì§€

ã€ì¶œë ¥ í˜•ì‹ã€‘
### ë¶ˆëŸ‰ í˜„í™©
(íŒì • ê²°ê³¼ ìš”ì•½)

### ì›ì¸ ë¶„ì„  
(ë§¤ë‰´ì–¼ ì›ì¸ ì¸ìš©)

### ëŒ€ì‘ ë°©ì•ˆ
(ì¦‰ì‹œ ì¡°ì¹˜ 2-3ê°œ)

### ì˜ˆë°© ì¡°ì¹˜
(ì¬ë°œ ë°©ì§€ 2-3ê°œ)


ìœ„ 4ê°œ ì„¹ì…˜ë§Œ ì‘ì„±í•˜ê³  ì¢…ë£Œí•˜ì„¸ìš”. ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì˜ˆì‹œ ë¶ˆí•„ìš”.
"""
    
    return prompt
# =========================
# ëª¨ë¸ ë¡œë”
# =========================
@app.on_event("startup")
async def load_models_on_startup():
    global llm_name, llm_model, llm_tokenizer
    global vlm_name, vlm_model, vlm_processor

    # ---- LLM ----
    try:
        llm_name = os.getenv("LLM_MODEL", "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B")
        #llm_name = os.getenv("LLM_MODEL", "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct")
        print(f"ğŸ”„ LLM ë¡œë“œ ì‹œë„: {llm_name}")

        # í† í¬ë‚˜ì´ì €: fast ìš°ì„ , ì‹¤íŒ¨ ì‹œ slow
        try:
            llm_tokenizer = AutoTokenizer.from_pretrained(
                llm_name, use_fast=True, trust_remote_code=True
            )
            print("âœ… LLM í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ (fast)")
        except Exception as e:
            print(f"[WARN] LLM fast tokenizer ì‹¤íŒ¨: {e} â†’ slow ì¬ì‹œë„")
            llm_tokenizer = AutoTokenizer.from_pretrained(
                llm_name, use_fast=False, trust_remote_code=True
            )
            print("âœ… LLM í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ (slow)")

        llm_model = AutoModelForCausalLM.from_pretrained(
            llm_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            trust_remote_code=True,
        )

        print("âœ… LLM ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
        llm_name = None
        llm_model = None
        llm_tokenizer = None

    # ---- VLM (LLaVA) ----
    try:
        vlm_name = os.getenv("VLM_MODEL", "llava-hf/llava-1.5-7b-hf")
        print(f"ğŸ”„ VLM ë¡œë“œ ì‹œë„: {vlm_name}")

        vlm_model = LlavaForConditionalGeneration.from_pretrained(
            vlm_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
        )
        vlm_processor = AutoProcessor.from_pretrained(vlm_name)
        print("âœ… VLM ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ VLM ë¡œë“œ ê±´ë„ˆëœ€: {e}")
        vlm_name = None
        vlm_model = None
        vlm_processor = None

# =========================
# ë£¨íŠ¸/í—¬ìŠ¤
# =========================
@app.get("/")
def root():
    return {
        "service": "LLM/VLM Server",
        "models": {
            "llm": llm_name,
            "vlm": vlm_name,
        },
        "endpoints": ["/analyze", "/analyze_vlm", "/health"],
    }

@app.get("/health")
def health():
    return {
        "status": "healthy",
        "llm": {"name": llm_name, "loaded": llm_model is not None},
        "vlm": {"name": vlm_name, "loaded": vlm_model is not None},
    }

# =========================
# LLM ë¶„ì„
# =========================
# llm_server.pyì˜ analyze í•¨ìˆ˜ ìˆ˜ì •
@app.post("/analyze")
def analyze(req: AnalysisRequest):
    print("\n" + "="*60)
    print("[LLM ANALYZE] ìš”ì²­ ì‹œì‘")
    print("="*60)
    
    if llm_model is None or llm_tokenizer is None:
        print("[ERROR] LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        raise HTTPException(503, detail="LLM not loaded")

    # 1. í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = _build_prompt(req)
    print(f"\n[PROMPT] ê¸¸ì´: {len(prompt)} ë¬¸ì")
    print(f"[PROMPT] ì²« 200ì:\n{prompt[:200]}...")
    
    # 2. í† í¬ë‚˜ì´ì§•
    device = next(llm_model.parameters()).device
    print(f"\n[DEVICE] {device}")
    
    inputs = llm_tokenizer(prompt, return_tensors="pt").to(device)
    input_length = inputs['input_ids'].shape[1]
    print(f"[INPUT] í† í° ìˆ˜: {input_length}")

    # 3. ìƒì„± íŒŒë¼ë¯¸í„°
    do_sample = (req.temperature or 0) > 0
    gen_kwargs = dict(
        max_new_tokens=min(max(req.max_new_tokens, 16), 800),
        temperature=float(max(min(req.temperature, 1.5), 0.0)),
        do_sample=do_sample,
        repetition_penalty=1.3,
    )
    if do_sample:
        gen_kwargs.update(dict(top_p=0.9))
    
    print(f"\n[GEN_KWARGS]")
    print(f"  - max_new_tokens: {gen_kwargs['max_new_tokens']}")
    print(f"  - temperature: {gen_kwargs['temperature']}")
    print(f"  - do_sample: {do_sample}")
    print(f"  - repetition_penalty: {gen_kwargs['repetition_penalty']}")

    # 4. ìƒì„±
    print(f"\n[GENERATE] ì‹œì‘...")
    start_time = time.time()
    
    with torch.no_grad():
        output_ids = llm_model.generate(**inputs, **gen_kwargs)
    
    gen_time = time.time() - start_time
    output_length = output_ids.shape[1]
    print(f"[GENERATE] ì™„ë£Œ ({gen_time:.2f}ì´ˆ)")
    print(f"[OUTPUT] ì „ì²´ í† í° ìˆ˜: {output_length}")
    print(f"[OUTPUT] ìƒì„±ëœ í† í° ìˆ˜: {output_length - input_length}")

    # 5. ë””ì½”ë”©
    print(f"\n[DECODE] ì‹œì‘...")
    generated_ids = output_ids[0][input_length:]
    text = llm_tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    print(f"[DECODE] ì›ë³¸ ê¸¸ì´: {len(text)} ë¬¸ì")
    #print(f"[DECODE] ì›ë³¸ ì²« 300ì:\n{text[:300]}...")
    print(f"[DECODE] ì›ë³¸ ì²« 300ì:\n{text}...")
    
    # 6. í›„ì²˜ë¦¬
    print(f"\n[POST-PROCESS] ì‹œì‘...")
    
    # assistant ë¶„ë¦¬
    if "assistant" in text:
        before_len = len(text)
        text = text.split("assistant")[0].strip()
        print(f"  - 'assistant' ë¶„ë¦¬: {before_len} -> {len(text)} ë¬¸ì")
    
    # [íšŒì‚¬ ë¶„ë¦¬
    if "[íšŒì‚¬" in text:
        before_len = len(text)
        text = text.split("[íšŒì‚¬")[0].strip()
        print(f"  - '[íšŒì‚¬' ë¶„ë¦¬: {before_len} -> {len(text)} ë¬¸ì")
    
    # ì˜ˆë°© ì¡°ì¹˜ ì´í›„ ìë¥´ê¸°
    lines = text.split('\n')
    prevention_idx = -1
    for i, line in enumerate(lines):
        if "ì˜ˆë°© ì¡°ì¹˜" in line or "ì˜ˆë°©ì¡°ì¹˜" in line:
            prevention_idx = i
            print(f"  - 'ì˜ˆë°© ì¡°ì¹˜' ë°œê²¬: ë¼ì¸ {i}")
            break
    
    if prevention_idx > 0:
        before_lines = len(lines)
        text = '\n'.join(lines[:prevention_idx + 7])
        print(f"  - ì˜ˆë°© ì¡°ì¹˜ ì´í›„ ìë¥´ê¸°: {before_lines} -> {prevention_idx + 7} ë¼ì¸")
    
    print(f"\n[FINAL] ìµœì¢… ê¸¸ì´: {len(text)} ë¬¸ì")
    print(f"[FINAL] ìµœì¢… ë¼ì¸ ìˆ˜: {len(text.split(chr(10)))}")
    print(f"[FINAL] ì²« 500ì:\n{text[:500]}...")
    
    print("\n" + "="*60)
    print("[LLM ANALYZE] ìš”ì²­ ì™„ë£Œ")
    print("="*60 + "\n")
    
    return {
        "status": "success",
        "analysis": text,
        "model": llm_name,
        "used_temperature": gen_kwargs["temperature"],
        "max_new_tokens": gen_kwargs["max_new_tokens"],
        "debug_info": {
            "input_tokens": input_length,
            "output_tokens": output_length - input_length,
            "generation_time": f"{gen_time:.2f}s",
            "final_length": len(text)
        }
    }

# =========================
# VLM ë¶„ì„ (LLaVA)
# =========================
@app.post("/analyze_vlm")
def analyze_vlm(req: VLMAnalysisRequest):
    print("\n" + "="*60)
    print("[VLM ANALYZE] ìš”ì²­ ì‹œì‘")
    print("="*60)
    
    if vlm_model is None or vlm_processor is None:
        print("[ERROR] VLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        raise HTTPException(503, detail="VLM not loaded")
    
    if not os.path.exists(req.image_path):
        print(f"[ERROR] ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {req.image_path}")
        raise HTTPException(400, detail=f"image_path not found: {req.image_path}")

    try:
        # 1. ì´ë¯¸ì§€ ë¡œë“œ
        print(f"\n[IMAGE] ë¡œë“œ: {req.image_path}")
        img = Image.open(req.image_path).convert("RGB")
        print(f"[IMAGE] í¬ê¸°: {img.size}")

        # 2. í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        print(f"\n[PROMPT] ê¸¸ì´: {len(req.prompt)} ë¬¸ì")
        print(f"[PROMPT] ì²« 200ì:\n{req.prompt[:200]}...")
        
        try:
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": req.prompt.strip()},
                    ],
                }
            ]
            prompt_text = vlm_processor.apply_chat_template(
                messages, add_generation_prompt=True, tokenize=False
            )
            print("[TEMPLATE] Chat template ì ìš© ì„±ê³µ")
        except Exception as e:
            print(f"[TEMPLATE] Chat template ì‹¤íŒ¨, ê¸°ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©: {e}")
            prompt_text = req.prompt.strip()

        # 3. ì…ë ¥ ì¤€ë¹„
        print(f"\n[PROCESSOR] ì…ë ¥ ì¤€ë¹„ ì¤‘...")
        inputs = vlm_processor(images=img, text=prompt_text, return_tensors="pt").to(vlm_model.device)
        print(f"[PROCESSOR] ì™„ë£Œ")

        # 4. ìƒì„± íŒŒë¼ë¯¸í„°
        do_sample = (req.temperature or 0) > 0
        gen_kwargs = dict(
            max_new_tokens=min(max(req.max_new_tokens, 16), 1024),
            temperature=float(max(min(req.temperature, 1.5), 0.0)),
            do_sample=do_sample,
        )
        if do_sample:
            gen_kwargs.update(dict(top_p=0.9))
        
        print(f"\n[GEN_KWARGS]")
        print(f"  - max_new_tokens: {gen_kwargs['max_new_tokens']}")
        print(f"  - temperature: {gen_kwargs['temperature']}")
        print(f"  - do_sample: {do_sample}")

        # 5. ìƒì„±
        print(f"\n[GENERATE] ì‹œì‘...")
        start_time = time.time()
        
        with torch.no_grad():
            out = vlm_model.generate(**inputs, **gen_kwargs)
        
        gen_time = time.time() - start_time
        print(f"[GENERATE] ì™„ë£Œ ({gen_time:.2f}ì´ˆ)")

        # 6. ë””ì½”ë”©
        print(f"\n[DECODE] ì‹œì‘...")
        text = vlm_processor.batch_decode(out, skip_special_tokens=True)[0]
        
        print(f"[DECODE] ì›ë³¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        print(f"[DECODE] ì›ë³¸ ì „ì²´:\n{text}")
        
        print("\n" + "="*60)
        print("[VLM ANALYZE] ìš”ì²­ ì™„ë£Œ")
        print("="*60 + "\n")
        
        return {
            "status": "success",
            "analysis": text,
            "model": vlm_name,
            "used_temperature": gen_kwargs["temperature"],
            "max_new_tokens": gen_kwargs["max_new_tokens"],
            "debug_info": {
                "generation_time": f"{gen_time:.2f}s",
                "output_length": len(text)
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        print(f"\n[ERROR] VLM ì¶”ë¡  ì˜¤ë¥˜:")
        import traceback
        traceback.print_exc()
        raise HTTPException(500, detail=f"VLM inference error: {e}")

# =========================
# ì„œë²„ ì‹¤í–‰ (ê°œë°œìš©)
# =========================
if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "5001"))
    uvicorn.run("llm_server:app", host="0.0.0.0", port=port, reload=False)
