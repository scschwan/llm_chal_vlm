# llm_server.py  (LLM + VLM ë™ì‹œ ì§€ì› ë²„ì „)

import os
import time
from typing import Dict, List, Optional

import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from PIL import Image

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoProcessor,
    LlavaForConditionalGeneration,
)

# =========================
# FastAPI
# =========================
app = FastAPI(title="LLM/VLM Server", version="1.0")

# =========================
# ì „ì—­ ëª¨ë¸ í•¸ë“¤ (LLM)
# =========================
llm_name: Optional[str] = None
llm_model: Optional[AutoModelForCausalLM] = None
llm_tokenizer: Optional[AutoTokenizer] = None

# =========================
# ì „ì—­ ëª¨ë¸ í•¸ë“¤ (VLM - LLaVA)
# =========================
vlm_name: Optional[str] = None
vlm_model: Optional[LlavaForConditionalGeneration] = None
vlm_processor: Optional[AutoProcessor] = None

# =========================
# ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
# =========================
class AnalysisRequest(BaseModel):
    product: str
    defect_en: str
    defect_ko: str
    full_name_ko: str
    anomaly_score: float = 0.0
    is_anomaly: bool = False
    manual_context: Dict[str, List[str]] = {}
    max_new_tokens: int = 512
    temperature: float = 0.7

class VLMAnalysisRequest(BaseModel):
    image_path: str
    prompt: str
    max_new_tokens: int = 256
    temperature: float = 0.2

# =========================
# ìœ í‹¸: í”„ë¡¬í”„íŠ¸ ë¹Œë”(LLM)
# =========================
def _build_prompt(req: AnalysisRequest) -> str:
    """ê¹”ë”í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    # ë§¤ë‰´ì–¼ ì •ë³´ (ì´ë¯¸ ì •ë¦¬ëœ ë¦¬ìŠ¤íŠ¸)
    causes = req.manual_context.get("ì›ì¸", [])
    actions = req.manual_context.get("ì¡°ì¹˜", [])
    
    has_manual = bool(causes or actions)
    
    # íŒì • ìƒíƒœ
    if req.is_anomaly:
        status = f"ë¶ˆëŸ‰ ê²€ì¶œ (ì´ìƒì ìˆ˜: {req.anomaly_score:.4f})"
    else:
        status = f"ì •ìƒ ë²”ìœ„ (ì´ìƒì ìˆ˜: {req.anomaly_score:.4f})"
    
    prompt = f"""ë‹¹ì‹ ì€ ì œì¡° í’ˆì§ˆ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì•„ëž˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•œ ë³´ê³ ì„œë¥¼ ìž‘ì„±í•˜ì„¸ìš”.

ã€ê²€ì‚¬ ê²°ê³¼ã€‘
ì œí’ˆ: {req.product}
ë¶ˆëŸ‰: {req.defect_ko} ({req.defect_en})
íŒì •: {status}

ã€ë§¤ë‰´ì–¼ã€‘
"""
    
    if has_manual:
        if causes:
            prompt += "ë°œìƒ ì›ì¸:\n"
            for i, cause in enumerate(causes, 1):
                prompt += f"{i}. {cause}\n"
        
        if actions:
            prompt += "\nì¡°ì¹˜ ë°©ë²•:\n"
            for i, action in enumerate(actions, 1):
                prompt += f"{i}. {action}\n"
    else:
        prompt += "â€» ë§¤ë‰´ì–¼ ì •ë³´ ì—†ìŒ\n"
    
    prompt += """
ã€ì§€ì¹¨ã€‘
- ìœ„ ë§¤ë‰´ì–¼ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš© (ë”°ì˜´í‘œ ì‚¬ìš©)
- 4ê°œ ì„¹ì…˜ë§Œ ìž‘ì„± (ê° 2-3ë¬¸ìž¥)
- ì¶”ì¸¡ì´ë‚˜ ì˜ˆì‹œ ë°˜ë³µ ê¸ˆì§€

ã€ì¶œë ¥ í˜•ì‹ã€‘
### ë¶ˆëŸ‰ í˜„í™©
(íŒì • ê²°ê³¼ ìš”ì•½)

### ì›ì¸ ë¶„ì„  
(ë§¤ë‰´ì–¼ ì›ì¸ ì¸ìš©)

### ëŒ€ì‘ ë°©ì•ˆ
(ì¦‰ì‹œ ì¡°ì¹˜ 2-3ê°œ)

### ì˜ˆë°© ì¡°ì¹˜
(ìž¬ë°œ ë°©ì§€ 2-3ê°œ)


ìœ„ 4ê°œ ì„¹ì…˜ë§Œ ìž‘ì„±í•˜ê³  ì¢…ë£Œí•˜ì„¸ìš”. ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì˜ˆì‹œ ë¶ˆí•„ìš”.
"""
    
    return prompt
# =========================
# ëª¨ë¸ ë¡œë”
# =========================
@app.on_event("startup")
async def load_models_on_startup():
    global llm_name, llm_model, llm_tokenizer
    global vlm_name, vlm_model, vlm_processor

    # ---- LLM ----
    try:
        llm_name = os.getenv("LLM_MODEL", "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B")
        #llm_name = os.getenv("LLM_MODEL", "LGAI-EXAONE/EXAONE-4.0-1.2B")
        print(f"ðŸ”„ LLM ë¡œë“œ ì‹œë„: {llm_name}")

        # í† í¬ë‚˜ì´ì €: fast ìš°ì„ , ì‹¤íŒ¨ ì‹œ slow
        try:
            llm_tokenizer = AutoTokenizer.from_pretrained(
                llm_name, use_fast=True, trust_remote_code=True
            )
            print("âœ… LLM í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ (fast)")
        except Exception as e:
            print(f"[WARN] LLM fast tokenizer ì‹¤íŒ¨: {e} â†’ slow ìž¬ì‹œë„")
            llm_tokenizer = AutoTokenizer.from_pretrained(
                llm_name, use_fast=False, trust_remote_code=True
            )
            print("âœ… LLM í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ (slow)")

        llm_model = AutoModelForCausalLM.from_pretrained(
            llm_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            trust_remote_code=True,
        )
        print("âœ… LLM ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
        llm_name = None
        llm_model = None
        llm_tokenizer = None

    # ---- VLM (LLaVA) ----
    try:
        vlm_name = os.getenv("VLM_MODEL", "llava-hf/llava-1.5-7b-hf")
        print(f"ðŸ”„ VLM ë¡œë“œ ì‹œë„: {vlm_name}")

        vlm_model = LlavaForConditionalGeneration.from_pretrained(
            vlm_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
        )
        vlm_processor = AutoProcessor.from_pretrained(vlm_name)
        print("âœ… VLM ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ VLM ë¡œë“œ ê±´ë„ˆëœ€: {e}")
        vlm_name = None
        vlm_model = None
        vlm_processor = None

# =========================
# ë£¨íŠ¸/í—¬ìŠ¤
# =========================
@app.get("/")
def root():
    return {
        "service": "LLM/VLM Server",
        "models": {
            "llm": llm_name,
            "vlm": vlm_name,
        },
        "endpoints": ["/analyze", "/analyze_vlm", "/health"],
    }

@app.get("/health")
def health():
    return {
        "status": "healthy",
        "llm": {"name": llm_name, "loaded": llm_model is not None},
        "vlm": {"name": vlm_name, "loaded": vlm_model is not None},
    }

# =========================
# LLM ë¶„ì„
# =========================
# llm_server.pyì˜ analyze í•¨ìˆ˜ ìˆ˜ì •
@app.post("/analyze")
def analyze(req: AnalysisRequest):
    if llm_model is None or llm_tokenizer is None:
        raise HTTPException(503, detail="LLM not loaded")

    prompt = _build_prompt(req)
    device = next(llm_model.parameters()).device
    inputs = llm_tokenizer(prompt, return_tensors="pt").to(device)

    do_sample = (req.temperature or 0) > 0
    gen_kwargs = dict(
        max_new_tokens=min(max(req.max_new_tokens, 16), 800),  # ì¶©ë¶„ížˆ ê¸¸ê²Œ
        temperature=float(max(min(req.temperature, 1.5), 0.0)),
        do_sample=do_sample,
        repetition_penalty=1.3,  # âœ… ë°˜ë³µ ë” ì–µì œ
    )
    if do_sample:
        gen_kwargs.update(dict(top_p=0.9))

    with torch.no_grad():
        output_ids = llm_model.generate(**inputs, **gen_kwargs)

    # âœ… í”„ë¡¬í”„íŠ¸ ì œì™¸
    generated_ids = output_ids[0][inputs['input_ids'].shape[1]:]
    text = llm_tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    # âœ… ê°„ë‹¨í•œ í›„ì²˜ë¦¬
    text = text.split("assistant")[0].strip()
    text = text.split("[íšŒì‚¬")[0].strip()
    
    # âœ… ì˜ˆë°© ì¡°ì¹˜ ì´í›„ 4-5ì¤„ ì§€ë‚˜ë©´ ìžë¥´ê¸°
    lines = text.split('\n')
    prevention_idx = -1
    for i, line in enumerate(lines):
        if "ì˜ˆë°© ì¡°ì¹˜" in line or "ì˜ˆë°©ì¡°ì¹˜" in line:
            prevention_idx = i
            break
    
    if prevention_idx > 0:
        # ì˜ˆë°© ì¡°ì¹˜ + 5ì¤„ë§Œ
        text = '\n'.join(lines[:prevention_idx + 7])
    
    return {
        "status": "success",
        "analysis": text,
        "model": llm_name,
        "used_temperature": gen_kwargs["temperature"],
        "max_new_tokens": gen_kwargs["max_new_tokens"],
    }

# =========================
# VLM ë¶„ì„ (LLaVA)
# =========================
@app.post("/analyze_vlm")
def analyze_vlm(req: VLMAnalysisRequest):
    if vlm_model is None or vlm_processor is None:
        raise HTTPException(503, detail="VLM not loaded")
    if not os.path.exists(req.image_path):
        raise HTTPException(400, detail=f"image_path not found: {req.image_path}")

    try:
        img = Image.open(req.image_path).convert("RGB")

        # ì‹ í˜• Processor: ì±„íŒ… í…œí”Œë¦¿(ë©€í‹°ëª¨ë‹¬) ì§€ì›
        try:
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": req.prompt.strip()},
                    ],
                }
            ]
            prompt_text = vlm_processor.apply_chat_template(
                messages, add_generation_prompt=True, tokenize=False
            )
        except Exception:
            # êµ¬í˜• í˜¸í™˜: í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ
            prompt_text = req.prompt.strip()

        inputs = vlm_processor(images=img, text=prompt_text, return_tensors="pt").to(vlm_model.device)

        do_sample = (req.temperature or 0) > 0
        gen_kwargs = dict(
            max_new_tokens=min(max(req.max_new_tokens, 16), 1024),
            temperature=float(max(min(req.temperature, 1.5), 0.0)),
            do_sample=do_sample,
        )
        if do_sample:
            gen_kwargs.update(dict(top_p=0.9))

        with torch.no_grad():
            out = vlm_model.generate(**inputs, **gen_kwargs)

        text = vlm_processor.batch_decode(out, skip_special_tokens=True)[0]
        return {
            "status": "success",
            "analysis": text,
            "model": vlm_name,
            "used_temperature": gen_kwargs["temperature"],
            "max_new_tokens": gen_kwargs["max_new_tokens"],
        }
    except HTTPException:
        raise
    except Exception as e:
        import traceback; traceback.print_exc()
        raise HTTPException(500, detail=f"VLM inference error: {e}")

# =========================
# ì„œë²„ ì‹¤í–‰ (ê°œë°œìš©)
# =========================
if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "5001"))
    uvicorn.run("llm_server:app", host="0.0.0.0", port=port, reload=False)
