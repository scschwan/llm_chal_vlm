# llm_server.py  (LLM + VLM ë™ì‹œ ì§€ì› ë²„ì „)

import os
import time
from typing import Dict, List, Optional

import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from PIL import Image

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoProcessor,
    LlavaForConditionalGeneration,
)

# =========================
# FastAPI
# =========================
app = FastAPI(title="LLM/VLM Server", version="1.0")

# =========================
# ì „ì—­ ëª¨ë¸ í•¸ë“¤ (LLM)
# =========================
llm_name: Optional[str] = None
llm_model: Optional[AutoModelForCausalLM] = None
llm_tokenizer: Optional[AutoTokenizer] = None

# =========================
# ì „ì—­ ëª¨ë¸ í•¸ë“¤ (VLM - LLaVA)
# =========================
vlm_name: Optional[str] = None
vlm_model: Optional[LlavaForConditionalGeneration] = None
vlm_processor: Optional[AutoProcessor] = None

# =========================
# ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
# =========================
class AnalysisRequest(BaseModel):
    product: str
    defect_en: str
    defect_ko: str
    full_name_ko: str
    anomaly_score: float = 0.0
    is_anomaly: bool = False
    manual_context: Dict[str, List[str]] = {}
    max_new_tokens: int = 512
    temperature: float = 0.7

class VLMAnalysisRequest(BaseModel):
    image_path: str
    prompt: str
    max_new_tokens: int = 256
    temperature: float = 0.2

# =========================
# ìœ í‹¸: í”„ë¡¬í”„íŠ¸ ë¹Œë”(LLM)
# =========================
def _build_prompt(req: AnalysisRequest) -> str:
    causes = "\n".join([f"- {c}" for c in req.manual_context.get("ì›ì¸", [])])
    actions = "\n".join([f"- {a}" for a in req.manual_context.get("ì¡°ì¹˜", [])])

     # ë””ë²„ê¹… ë¡œê·¸
    print(f"[DEBUG] ë§¤ë‰´ì–¼ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì‹ :")
    print(f"  ì›ì¸ ê°œìˆ˜: {len(req.manual_context.get('ì›ì¸', []))}")
    print(f"  ì¡°ì¹˜ ê°œìˆ˜: {len(req.manual_context.get('ì¡°ì¹˜', []))}")
    
    if not causes.strip() and not actions.strip():
        print("âš ï¸  ë§¤ë‰´ì–¼ ì •ë³´ê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤!")
        
    manual_present = bool(causes.strip() or actions.strip())
    manual_block = f"""
### ë°œìƒ ì›ì¸(ë§¤ë‰´ì–¼ ë°œì·Œ)
{causes if causes else "ë§¤ë‰´ì–¼ ì •ë³´ ì—†ìŒ"}

### ì¡°ì¹˜ ê°€ì´ë“œ(ë§¤ë‰´ì–¼ ë°œì·Œ)
{actions if actions else "ë§¤ë‰´ì–¼ ì •ë³´ ì—†ìŒ"}
""".strip()

    policy = (
        "ë°˜ë“œì‹œ ìœ„ì˜ 'ë§¤ë‰´ì–¼ ë°œì·Œ'ë¥¼ 1ì°¨ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ê³ , ë‹¤ë¥¸ ì¶”ì •ì€ ê¸ˆì§€í•˜ì„¸ìš”."
        if manual_present else
        "ë§¤ë‰´ì–¼ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ í•©ë¦¬ì  ê°€ì •ì„ ëª…ì‹œì ìœ¼ë¡œ í‘œê¸°í•´ ì œì‹œí•˜ì„¸ìš”."
    )

    prompt = f"""ë‹¹ì‹ ì€ ì œì¡°ì—… í’ˆì§ˆ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì•„ëž˜ ë¶ˆëŸ‰ ì •ë³´ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

## ë¶ˆëŸ‰ ì •ë³´
- ì œí’ˆ: {req.product}
- ë¶ˆëŸ‰ ìœ í˜•: {req.defect_ko} ({req.defect_en})
- ì •ì‹ ëª…ì¹­: {req.full_name_ko}
- ì´ìƒ ê²€ì¶œ ì ìˆ˜: {req.anomaly_score:.4f}
- ë¶ˆëŸ‰ íŒì •: {"ë¶ˆëŸ‰" if req.is_anomaly else "ì •ìƒ"}

{manual_block}

## ìž‘ì„± ê·œì¹™
1) {policy}
2) ë§¤ë‰´ì–¼ ê·¼ê±° ë¬¸ìž¥ì„ "ë”°ì˜´í‘œ"ë¡œ ì¸ìš©í•˜ê³ , í•­ëª©ë³„ë¡œ ë§¤í•‘í•´ ì£¼ì„¸ìš”.
3) [ë¶ˆëŸ‰ í˜„í™© ìš”ì•½] â†’ [ì›ì¸ ë¶„ì„] â†’ [ëŒ€ì‘ ë°©ì•ˆ] â†’ [ì˜ˆë°© ì¡°ì¹˜] ìˆœìœ¼ë¡œ ìž‘ì„±.

## ì¶œë ¥:
- ë¶ˆë¦¿/ë²ˆí˜¸ ëª©ë¡ ìœ„ì£¼ë¡œ ê°„ê²°í•˜ê²Œ.
""".strip()
    return prompt

# =========================
# ëª¨ë¸ ë¡œë”
# =========================
@app.on_event("startup")
async def load_models_on_startup():
    global llm_name, llm_model, llm_tokenizer
    global vlm_name, vlm_model, vlm_processor

    # ---- LLM ----
    try:
        llm_name = os.getenv("LLM_MODEL", "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B")
        print(f"ðŸ”„ LLM ë¡œë“œ ì‹œë„: {llm_name}")

        # í† í¬ë‚˜ì´ì €: fast ìš°ì„ , ì‹¤íŒ¨ ì‹œ slow
        try:
            llm_tokenizer = AutoTokenizer.from_pretrained(
                llm_name, use_fast=True, trust_remote_code=True
            )
            print("âœ… LLM í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ (fast)")
        except Exception as e:
            print(f"[WARN] LLM fast tokenizer ì‹¤íŒ¨: {e} â†’ slow ìž¬ì‹œë„")
            llm_tokenizer = AutoTokenizer.from_pretrained(
                llm_name, use_fast=False, trust_remote_code=True
            )
            print("âœ… LLM í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ (slow)")

        llm_model = AutoModelForCausalLM.from_pretrained(
            llm_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            trust_remote_code=True,
        )
        print("âœ… LLM ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
        llm_name = None
        llm_model = None
        llm_tokenizer = None

    # ---- VLM (LLaVA) ----
    try:
        vlm_name = os.getenv("VLM_MODEL", "llava-hf/llava-1.5-7b-hf")
        print(f"ðŸ”„ VLM ë¡œë“œ ì‹œë„: {vlm_name}")

        vlm_model = LlavaForConditionalGeneration.from_pretrained(
            vlm_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
        )
        vlm_processor = AutoProcessor.from_pretrained(vlm_name)
        print("âœ… VLM ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ VLM ë¡œë“œ ê±´ë„ˆëœ€: {e}")
        vlm_name = None
        vlm_model = None
        vlm_processor = None

# =========================
# ë£¨íŠ¸/í—¬ìŠ¤
# =========================
@app.get("/")
def root():
    return {
        "service": "LLM/VLM Server",
        "models": {
            "llm": llm_name,
            "vlm": vlm_name,
        },
        "endpoints": ["/analyze", "/analyze_vlm", "/health"],
    }

@app.get("/health")
def health():
    return {
        "status": "healthy",
        "llm": {"name": llm_name, "loaded": llm_model is not None},
        "vlm": {"name": vlm_name, "loaded": vlm_model is not None},
    }

# =========================
# LLM ë¶„ì„
# =========================
@app.post("/analyze")
def analyze(req: AnalysisRequest):
    if llm_model is None or llm_tokenizer is None:
        raise HTTPException(503, detail="LLM not loaded")

    prompt = _build_prompt(req)

    # device ì¶”ì¶œ (device_map="auto"ì¼ ë•Œë„ ì²« íŒŒë¼ë¯¸í„°ì˜ device ì‚¬ìš©)
    try:
        device = next(llm_model.parameters()).device
    except StopIteration:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    inputs = llm_tokenizer(prompt, return_tensors="pt").to(device)

    do_sample = (req.temperature or 0) > 0
    gen_kwargs = dict(
        max_new_tokens=min(max(req.max_new_tokens, 16), 2048),
        temperature=float(max(min(req.temperature, 1.5), 0.0)),
        do_sample=do_sample,
    )
    if do_sample:
        gen_kwargs.update(dict(top_p=0.9))

    with torch.no_grad():
        output_ids = llm_model.generate(**inputs, **gen_kwargs)

    text = llm_tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return {
        "status": "success",
        "analysis": text,
        "model": llm_name,
        "used_temperature": gen_kwargs["temperature"],
        "max_new_tokens": gen_kwargs["max_new_tokens"],
    }

# =========================
# VLM ë¶„ì„ (LLaVA)
# =========================
@app.post("/analyze_vlm")
def analyze_vlm(req: VLMAnalysisRequest):
    if vlm_model is None or vlm_processor is None:
        raise HTTPException(503, detail="VLM not loaded")
    if not os.path.exists(req.image_path):
        raise HTTPException(400, detail=f"image_path not found: {req.image_path}")

    try:
        img = Image.open(req.image_path).convert("RGB")

        # ì‹ í˜• Processor: ì±„íŒ… í…œí”Œë¦¿(ë©€í‹°ëª¨ë‹¬) ì§€ì›
        try:
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": req.prompt.strip()},
                    ],
                }
            ]
            prompt_text = vlm_processor.apply_chat_template(
                messages, add_generation_prompt=True, tokenize=False
            )
        except Exception:
            # êµ¬í˜• í˜¸í™˜: í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ
            prompt_text = req.prompt.strip()

        inputs = vlm_processor(images=img, text=prompt_text, return_tensors="pt").to(vlm_model.device)

        do_sample = (req.temperature or 0) > 0
        gen_kwargs = dict(
            max_new_tokens=min(max(req.max_new_tokens, 16), 1024),
            temperature=float(max(min(req.temperature, 1.5), 0.0)),
            do_sample=do_sample,
        )
        if do_sample:
            gen_kwargs.update(dict(top_p=0.9))

        with torch.no_grad():
            out = vlm_model.generate(**inputs, **gen_kwargs)

        text = vlm_processor.batch_decode(out, skip_special_tokens=True)[0]
        return {
            "status": "success",
            "analysis": text,
            "model": vlm_name,
            "used_temperature": gen_kwargs["temperature"],
            "max_new_tokens": gen_kwargs["max_new_tokens"],
        }
    except HTTPException:
        raise
    except Exception as e:
        import traceback; traceback.print_exc()
        raise HTTPException(500, detail=f"VLM inference error: {e}")

# =========================
# ì„œë²„ ì‹¤í–‰ (ê°œë°œìš©)
# =========================
if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "5001"))
    uvicorn.run("llm_server:app", host="0.0.0.0", port=port, reload=False)
