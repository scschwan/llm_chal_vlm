# llm_server.py
"""
LLM ì „ìš© API ì„œë²„
ë³„ë„ ê°€ìƒí™˜ê²½(venv_llm)ì—ì„œ ì‹¤í–‰
tokenizers 0.15.2 ì‚¬ìš©
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Optional
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig , AutoProcessor
import uvicorn
import os

app = FastAPI(title="LLM Server", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ëª¨ë¸
model = None
tokenizer = None
model_name = None

vlm_model = None
vlm_processor = None
vlm_name = None


class AnalysisRequest(BaseModel):
    """ë¶ˆëŸ‰ ë¶„ì„ ìš”ì²­"""
    product: str
    defect_en: str
    defect_ko: str
    full_name_ko: str
    anomaly_score: float
    is_anomaly: bool
    manual_context: Dict[str, List[str]]
    max_new_tokens: int = 512
    temperature: float = 0.7


class AnalysisResponse(BaseModel):
    """ë¶ˆëŸ‰ ë¶„ì„ ì‘ë‹µ"""
    status: str
    analysis: str
    model: str


@app.on_event("startup")
async def load_model():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global model, tokenizer, model_name
    
    # í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’
    model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"
    
    print("=" * 60)
    print(f"ğŸ¤– LLM ì„œë²„ ì‹œì‘ ì¤‘...")
    print(f"ğŸ“¦ ëª¨ë¸: {model_name}")
    print("=" * 60)
    
    try:
        # ì–‘ìí™” ì„¤ì •
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        print("ğŸ”„ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
        try :
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                use_fast=True,
                local_files_only=False,
                force_download=True,   # ìºì‹œê°€ ì´ìƒí•˜ë©´ ìƒˆë¡œ ë°›ê¸°
            )
            print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
        except Exception as e :
            print(f"[WARN] Fast tokenizer failed: {e}\n--> Falling back to slow tokenizer.")
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                use_fast=False,
                local_files_only=False,
                force_download=True,
            )
            print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ (slow)")
        
        print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘ (4-bit ì–‘ìí™”)...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            trust_remote_code=True,
            device_map="auto"
        )
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        print("=" * 60)
        print("âœ… LLM ì„œë²„ ì¤€ë¹„ ì™„ë£Œ")
        print(f"ğŸŒ í¬íŠ¸: 5001")
        print("=" * 60)
        
        try:
            vlm_name = os.getenv("VLM_MODEL", "llava-hf/llava-1.5-7b-hf")  # ì˜ˆì‹œ
            print(f"ğŸ”„ VLM ë¡œë“œ ì‹œë„: {vlm_name}")
            global vlm_model, vlm_processor
            vlm_processor = AutoProcessor.from_pretrained(vlm_name, trust_remote_code=True)
            vlm_model = AutoModelForCausalLM.from_pretrained(
                vlm_name, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True
            )
            print("âœ… VLM ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ VLM ë¡œë“œ ê±´ë„ˆëœ€: {e}")
            vlm_model = None
            vlm_processor = None
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


@app.post("/analyze", response_model=AnalysisResponse)
async def analyze(request: AnalysisRequest):
    """ë¶ˆëŸ‰ ë¶„ì„ ìˆ˜í–‰"""
    
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = _build_prompt(request)
        
        # HyperCLOVA-X í˜•ì‹ìœ¼ë¡œ ìƒì„±
        chat = [
            {"role": "tool_list", "content": ""},
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì œì¡°ì—… í’ˆì§ˆ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
        
        inputs = tokenizer.apply_chat_template(
            chat,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        inputs = inputs.to(model.device)
        
        # ìƒì„±
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_length=request.max_new_tokens + inputs["input_ids"].shape[1],
                stop_strings=["<|endofturn|>", "<|stop|>"],
                tokenizer=tokenizer,
                temperature=request.temperature,
                do_sample=True if request.temperature > 0 else False
            )
        
        # ë””ì½”ë”©
        generated_text = tokenizer.decode(
            output_ids[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return AnalysisResponse(
            status="success",
            analysis=generated_text.strip(),
            model=model_name
        )
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

class VLMAnalysisRequest(BaseModel):
    image_path: str
    prompt: str
    max_new_tokens: int = 256
    temperature: float = 0.2

@app.post("/analyze_vlm")
async def analyze_vlm(req: VLMAnalysisRequest):
    if vlm_model is None or vlm_processor is None:
        raise HTTPException(503, detail="VLM not loaded")
    try:
        from PIL import Image
        img = Image.open(req.image_path).convert("RGB")
        inputs = vlm_processor(images=img, text=req.prompt, return_tensors="pt").to(vlm_model.device)
        with torch.no_grad():
            out_ids = vlm_model.generate(
                **inputs, max_new_tokens=req.max_new_tokens,
                temperature=req.temperature, do_sample=req.temperature > 0
            )
        text = vlm_processor.batch_decode(out_ids, skip_special_tokens=True)[0]
        return {"status": "success", "analysis": text, "model": vlm_name}
    except Exception as e:
        raise HTTPException(500, detail=str(e))

def _build_prompt(request: AnalysisRequest) -> str:
    causes = "\n".join([f"- {c}" for c in request.manual_context.get("ì›ì¸", [])])
    actions = "\n".join([f"- {a}" for a in request.manual_context.get("ì¡°ì¹˜", [])])

    manual_present = bool(causes.strip() or actions.strip())
    manual_block = f"""
    ### ë°œìƒ ì›ì¸(ë§¤ë‰´ì–¼ ë°œì·Œ)
    {causes if causes else "ë§¤ë‰´ì–¼ ì •ë³´ ì—†ìŒ"}

    ### ì¡°ì¹˜ ê°€ì´ë“œ(ë§¤ë‰´ì–¼ ë°œì·Œ)
    {actions if actions else "ë§¤ë‰´ì–¼ ì •ë³´ ì—†ìŒ"}
    """

    policy = (
      "ë°˜ë“œì‹œ ìœ„ì˜ 'ë§¤ë‰´ì–¼ ë°œì·Œ'ë¥¼ 1ì°¨ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ê³ , ë‹¤ë¥¸ ì¶”ì •ì€ ê¸ˆì§€í•˜ì„¸ìš”."
      if manual_present else
      "ë§¤ë‰´ì–¼ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ í•©ë¦¬ì  ê°€ì •ì„ ëª…ì‹œì ìœ¼ë¡œ í‘œê¸°í•´ ì œì‹œí•˜ì„¸ìš”."
    )

    prompt = f"""ë‹¹ì‹ ì€ ì œì¡°ì—… í’ˆì§ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë¶ˆëŸ‰ ì •ë³´ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

    ## ë¶ˆëŸ‰ ì •ë³´
    - ì œí’ˆ: {request.product}
    - ë¶ˆëŸ‰ ìœ í˜•: {request.defect_ko} ({request.defect_en})
    - ì •ì‹ ëª…ì¹­: {request.full_name_ko}
    - ì´ìƒ ê²€ì¶œ ì ìˆ˜: {request.anomaly_score:.4f}
    - ë¶ˆëŸ‰ íŒì •: {"ë¶ˆëŸ‰" if request.is_anomaly else "ì •ìƒ"}

    {manual_block}

    ## ì‘ì„± ê·œì¹™
    1) {policy}
    2) ë§¤ë‰´ì–¼ ê·¼ê±° ë¬¸ì¥ì„ "ë”°ì˜´í‘œ"ë¡œ ì¸ìš©í•˜ê³ , í•­ëª©ë³„ë¡œ ë§¤í•‘í•´ ì£¼ì„¸ìš”.
    3) [ë¶ˆëŸ‰ í˜„í™© ìš”ì•½] â†’ [ì›ì¸ ë¶„ì„] â†’ [ëŒ€ì‘ ë°©ì•ˆ] â†’ [ì˜ˆë°© ì¡°ì¹˜] ìˆœìœ¼ë¡œ ì‘ì„±.

    ## ì¶œë ¥:
    - ë¶ˆë¦¿/ë²ˆí˜¸ ëª©ë¡ ìœ„ì£¼ë¡œ ê°„ê²°í•˜ê²Œ.
    """
    return prompt


@app.get("/health")
async def health():
    """í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "model": model_name,
        "model_loaded": model is not None
    }


@app.get("/")
async def root():
    """ë£¨íŠ¸"""
    return {
        "service": "LLM Server",
        "model": model_name,
        "endpoints": ["/analyze", "/health"]
    }


if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=5001,
        log_level="info"
    )