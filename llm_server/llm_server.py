# llm_server.py - EXAONE 3.5 ë° HyperCLOVAX ì§€ì›

import os
import time
from typing import Dict, List, Optional, Tuple
from enum import Enum
import uvicorn
import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from PIL import Image

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoProcessor,
    LlavaForConditionalGeneration,
)

# =========================
# FastAPI
# =========================
app = FastAPI(title="LLM/VLM Server", version="2.0")

# =========================
# ëª¨ë¸ íƒ€ìž… ì •ì˜
# =========================
class LLMProvider(str, Enum):
    HYPERCLOVAX = "hyperclovax"
    EXAONE = "exaone"



# =========================
# ì „ì—­ ëª¨ë¸ í•¸ë“¤
# =========================

hyperclovax_model: Optional[AutoModelForCausalLM] = None
hyperclovax_tokenizer: Optional[AutoTokenizer] = None

exaone_model: Optional[AutoModelForCausalLM] = None
exaone_tokenizer: Optional[AutoTokenizer] = None

vlm_name: Optional[str] = None
vlm_model: Optional[LlavaForConditionalGeneration] = None
vlm_processor: Optional[AutoProcessor] = None

# =========================
# ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
# =========================
class AnalysisRequest(BaseModel):
    product: str
    defect_en: str
    defect_ko: str
    full_name_ko: str
    anomaly_score: float = 0.0
    is_anomaly: bool = False
    manual_context: Dict[str, List[str]] = {}
    max_new_tokens: int = 512
    temperature: float = 0.7
    model_provider: Optional[str] = None  # 'hyperclovax' ë˜ëŠ” 'exaone'

class VLMAnalysisRequest(BaseModel):
    image_path: str
    prompt: str
    max_new_tokens: int = 256
    temperature: float = 0.2


# =========================
# í”„ë¡¬í”„íŠ¸ ë¹Œë”
# =========================
def _build_prompt_text(req: AnalysisRequest) -> str:
    """í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ìƒì„± (ê³µí†µ)"""
    causes = req.manual_context.get("ì›ì¸", [])
    actions = req.manual_context.get("ì¡°ì¹˜", [])
    has_manual = bool(causes or actions)
    
    if req.is_anomaly:
        status = f"ë¶ˆëŸ‰ ê²€ì¶œ (ì´ìƒì ìˆ˜: {req.anomaly_score:.4f})"
    else:
        status = f"ì •ìƒ ë²”ìœ„ (ì´ìƒì ìˆ˜: {req.anomaly_score:.4f})"
    
    prompt = f"""ë‹¹ì‹ ì€ ì œì¡° í’ˆì§ˆ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì•„ëž˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•œ ë³´ê³ ì„œë¥¼ ìž‘ì„±í•˜ì„¸ìš”.

ã€ê²€ì‚¬ ê²°ê³¼ã€‘
ì œí’ˆ: {req.product}
ë¶ˆëŸ‰: {req.defect_ko} ({req.defect_en})
íŒì •: {status}

ã€ë§¤ë‰´ì–¼ã€‘
"""
    
    if has_manual:
        if causes:
            prompt += "ë°œìƒ ì›ì¸:\n"
            for i, cause in enumerate(causes, 1):
                prompt += f"{i}. {cause}\n"
        
        if actions:
            prompt += "\nì¡°ì¹˜ ë°©ë²•:\n"
            for i, action in enumerate(actions, 1):
                prompt += f"{i}. {action}\n"
    else:
        prompt += "â€» ë§¤ë‰´ì–¼ ì •ë³´ ì—†ìŒ\n"
    
    prompt += """
ã€ì§€ì¹¨ã€‘
- ìœ„ ë§¤ë‰´ì–¼ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš© (ë”°ì˜´í‘œ ì‚¬ìš©)
- 4ê°œ ì„¹ì…˜ë§Œ ìž‘ì„± (ê° 2-3ë¬¸ìž¥)
- ì¶”ì¸¡ì´ë‚˜ ì˜ˆì‹œ ë°˜ë³µ ê¸ˆì§€

ã€ì¶œë ¥ í˜•ì‹ã€‘
### ë¶ˆëŸ‰ í˜„í™©
(íŒì • ê²°ê³¼ ìš”ì•½)

### ì›ì¸ ë¶„ì„  
(ë§¤ë‰´ì–¼ ì›ì¸ ì¸ìš©)

### ëŒ€ì‘ ë°©ì•ˆ
(ì¦‰ì‹œ ì¡°ì¹˜ 2-3ê°œ)

### ì˜ˆë°© ì¡°ì¹˜
(ìž¬ë°œ ë°©ì§€ 2-3ê°œ)

ìœ„ 4ê°œ ì„¹ì…˜ë§Œ ìž‘ì„±í•˜ê³  ì¢…ë£Œí•˜ì„¸ìš”. ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì˜ˆì‹œ ë¶ˆí•„ìš”.
"""
    return prompt

def _prepare_inputs_hyperclovax(prompt_text: str, tokenizer):
    """HyperCLOVAXìš© ìž…ë ¥ ì¤€ë¹„"""
    # ë‹¨ìˆœ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
    return tokenizer(prompt_text, return_tensors="pt")

def _prepare_inputs_exaone(prompt_text: str, tokenizer):
    """EXAONE 3.5ìš© ìž…ë ¥ ì¤€ë¹„ (chat template ì‚¬ìš©)"""
    messages = [
        {
            "role": "system", 
            "content": "You are EXAONE model from LG AI Research, a helpful assistant specialized in manufacturing quality control."
        },
        {
            "role": "user",
            "content": prompt_text
        }
    ]
    
    # Chat template ì ìš©
    input_ids = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    )
    
    return {"input_ids": input_ids}

# =========================
# ëª¨ë¸ ì´ˆê¸°í™” (ìˆ˜ì •)
# =========================
@app.on_event("startup")
async def load_models_on_startup():
    global hyperclovax_model, hyperclovax_tokenizer
    global exaone_model, exaone_tokenizer
    global vlm_name, vlm_model, vlm_processor
    
    print("=" * 60)
    print("LLM/VLM ì„œë²„ ì‹œìž‘")
    print("=" * 60)
    
    # 1. HyperCLOVAX ë¡œë“œ
    print("\n[1/3] HyperCLOVAX ë¡œë“œ ì¤‘...")
    try:
        model_id = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"
        print(f"ðŸ”„ ë¡œë“œ ì‹œë„: {model_id}")
        
        try:
            hyperclovax_tokenizer = AutoTokenizer.from_pretrained(
                model_id, use_fast=True, trust_remote_code=True
            )
        except:
            hyperclovax_tokenizer = AutoTokenizer.from_pretrained(
                model_id, use_fast=False, trust_remote_code=True
            )
        
        hyperclovax_model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            trust_remote_code=True,
        )
        print("âœ… HyperCLOVAX ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ HyperCLOVAX ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 2. EXAONE 3.5 ë¡œë“œ
    print("\n[2/3] EXAONE 3.5 ë¡œë“œ ì¤‘...")
    try:
        model_id = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
        print(f"ðŸ”„ ë¡œë“œ ì‹œë„: {model_id}")
        
        try:
            exaone_tokenizer = AutoTokenizer.from_pretrained(
                model_id, use_fast=True, trust_remote_code=True
            )
        except:
            exaone_tokenizer = AutoTokenizer.from_pretrained(
                model_id, use_fast=False, trust_remote_code=True
            )
        
        exaone_model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            trust_remote_code=True,
        )
        print("âœ… EXAONE 3.5 ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ EXAONE ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 3. VLM ë¡œë“œ
    print("\n[3/3] VLM ë¡œë“œ ì¤‘...")
    try:
        vlm_name = os.getenv("VLM_MODEL", "llava-hf/llava-1.5-7b-hf")
        print(f"ðŸ”„ VLM ë¡œë“œ ì‹œë„: {vlm_name}")

        vlm_model = LlavaForConditionalGeneration.from_pretrained(
            vlm_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
        )
        vlm_processor = AutoProcessor.from_pretrained(vlm_name)
        print("âœ… VLM ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ VLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 60)
    print("âœ… ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ")
    print(f"  - HyperCLOVAX: {'ë¡œë“œë¨' if hyperclovax_model else 'ì‹¤íŒ¨'}")
    print(f"  - EXAONE 3.5: {'ë¡œë“œë¨' if exaone_model else 'ì‹¤íŒ¨'}")
    print(f"  - VLM: {'ë¡œë“œë¨' if vlm_model else 'ì‹¤íŒ¨'}")
    print("=" * 60 + "\n")



# =========================
# LLM ë¶„ì„ (ìˆ˜ì •)
# =========================
@app.post("/analyze")
def analyze(req: AnalysisRequest):
    """
    LLM ê¸°ë°˜ ë¶„ì„
    
    Args:
        req.model_provider: 'hyperclovax' ë˜ëŠ” 'exaone' (ê¸°ë³¸ê°’: hyperclovax)
    """
    # ëª¨ë¸ ì„ íƒ
    provider = req.model_provider or LLMProvider.HYPERCLOVAX
    
    if provider == LLMProvider.EXAONE:
        if exaone_model is None or exaone_tokenizer is None:
            raise HTTPException(503, "EXAONE ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        llm_model = exaone_model
        llm_tokenizer = exaone_tokenizer
    else:  # HYPERCLOVAX
        if hyperclovax_model is None or hyperclovax_tokenizer is None:
            raise HTTPException(503, "HyperCLOVAX ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        llm_model = hyperclovax_model
        llm_tokenizer = hyperclovax_tokenizer
    
    # í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
    prompt_text = _build_prompt_text(req)
    
    # ëª¨ë¸ë³„ ìž…ë ¥ ì¤€ë¹„
    if provider == LLMProvider.EXAONE:
        inputs = _prepare_inputs_exaone(prompt_text, llm_tokenizer)
    else:  # HYPERCLOVAX
        inputs = _prepare_inputs_hyperclovax(prompt_text, llm_tokenizer)
    
    # GPUë¡œ ì´ë™
    device = next(llm_model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # ìƒì„± íŒŒë¼ë¯¸í„°
    do_sample = (req.temperature or 0) > 0
    gen_kwargs = {
        "max_new_tokens": min(max(req.max_new_tokens, 16), 800),
        "temperature": float(max(min(req.temperature, 1.5), 0.0)),
        "do_sample": do_sample,
        "repetition_penalty": 1.3,
    }
    
    if do_sample:
        gen_kwargs["top_p"] = 0.9
    
    # EXAONEì€ eos_token_id ëª…ì‹œ
    if provider == LLMProvider.EXAONE:
        gen_kwargs["eos_token_id"] = llm_tokenizer.eos_token_id

    # ì¶”ë¡ 
    with torch.no_grad():
        output_ids = llm_model.generate(**inputs, **gen_kwargs)

    # ë””ì½”ë”© (í”„ë¡¬í”„íŠ¸ ì œì™¸)
    if provider == LLMProvider.EXAONE:
        # EXAONE: ì „ì²´ ì¶œë ¥ ë””ì½”ë”© í›„ íŒŒì‹±
        full_text = llm_tokenizer.decode(output_ids[0], skip_special_tokens=True)
        
        # ASSISTANT ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "ASSISTANT:" in full_text:
            text = full_text.split("ASSISTANT:")[-1].strip()
        else:
            text = full_text
    else:
        # HyperCLOVAX: í”„ë¡¬í”„íŠ¸ ì œì™¸
        generated_ids = output_ids[0][inputs['input_ids'].shape[1]:]
        text = llm_tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    # í›„ì²˜ë¦¬
    text = text.split("assistant")[0].strip()
    text = text.split("[íšŒì‚¬")[0].strip()
    
    # ì˜ˆë°© ì¡°ì¹˜ ì´í›„ ìžë¥´ê¸°
    lines = text.split('\n')
    prevention_idx = -1
    for i, line in enumerate(lines):
        if "ì˜ˆë°© ì¡°ì¹˜" in line or "ì˜ˆë°©ì¡°ì¹˜" in line:
            prevention_idx = i
            break
    
    if prevention_idx > 0:
        text = '\n'.join(lines[:prevention_idx + 7])
    
    return {
        "status": "success",
        "analysis": text,
        "model": "EXAONE-3.5" if provider == LLMProvider.EXAONE else "HyperCLOVAX",
        "model_provider": provider,
        "used_temperature": gen_kwargs["temperature"],
        "max_new_tokens": gen_kwargs["max_new_tokens"],
    }

# =========================
# VLM ë¶„ì„ (ë³€ê²½ ì—†ìŒ)
# =========================
@app.post("/analyze_vlm")
def analyze_vlm(req: VLMAnalysisRequest):
    if vlm_model is None or vlm_processor is None:
        raise HTTPException(503, detail="VLM not loaded")
    if not os.path.exists(req.image_path):
        raise HTTPException(400, detail=f"image_path not found: {req.image_path}")

    try:
        img = Image.open(req.image_path).convert("RGB")

        try:
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": req.prompt.strip()},
                    ],
                }
            ]
            prompt_text = vlm_processor.apply_chat_template(
                messages, add_generation_prompt=True, tokenize=False
            )
        except Exception:
            prompt_text = req.prompt.strip()

        inputs = vlm_processor(images=img, text=prompt_text, return_tensors="pt").to(vlm_model.device)

        do_sample = (req.temperature or 0) > 0
        gen_kwargs = dict(
            max_new_tokens=min(max(req.max_new_tokens, 16), 1024),
            temperature=float(max(min(req.temperature, 1.5), 0.0)),
            do_sample=do_sample,
        )
        if do_sample:
            gen_kwargs.update(dict(top_p=0.9))

        with torch.no_grad():
            out = vlm_model.generate(**inputs, **gen_kwargs)

        text = vlm_processor.batch_decode(out, skip_special_tokens=True)[0]
        return {
            "status": "success",
            "analysis": text,
            "model": vlm_name,
            "used_temperature": gen_kwargs["temperature"],
            "max_new_tokens": gen_kwargs["max_new_tokens"],
        }
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, detail=f"VLM inference error: {e}")

# =========================
# ì„œë²„ ì‹¤í–‰
# =========================
if __name__ == "__main__":

    port = int(os.getenv("PORT", "5001"))
    uvicorn.run("llm_server:app", host="0.0.0.0", port=port, reload=False)